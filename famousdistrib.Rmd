--- 
title: "Visualizing Probability Distributions with ggplot"
author: "Ben Wallace"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width='80%', fig.asp=.75, fig.align='center')
```

# Introduction

This is a guide to understanding and visualizing several important discrete and continuous distributions in the statistics world. We will use several R packages in the process.

- `stats` (installed by default in RStudio) to retrieve statistical distributions.

- `tidyverse` to coerce data into [tidy](https://r4ds.had.co.nz/tibbles.html) format and produce visualizations with `ggplot`.

```{r eval=FALSE}
install.packages("tidyverse") # Includes both tibble and ggplot2 packages
# or the development version
# devtools::install_github("tidyverse")
```

I created the book primarily for those that are new to statistics and/or data science and are somewhat familiar with the R language. If you would like a more comprehensive introduction to R, I recommend both of these free and accessible books:

1. [R for Data Science](https://r4ds.had.co.nz/)

2. [Learning Statistics with R](https://learningstatisticswithr.com/)

I find it helpful to begin this guide with an overview of the function `ggplot`, which will be the first chapter. This includes an important conceptual basis for aesthetics, geometries, mappings, and scales.The lessons learned in ggplot chapter will then be applied to the remainder of the book. In each chapter, we will introduce a statistical distribution and use ggplot to better visualize our concepts. 

1. **Basics of ggplot**: Geometries and Aesthetic Mappings, Altering Aesthetics and Scales.

2. **Intro to Distribution Theory**: Uniform Distribution.

3. **Discrete Distributions**: Geometric, Binomial, Negative Binomial, Poisson Distributions.

4. **From Discrete to Continuous Cases**: Normal Approximation and Demoivre's Problem, Normal Distribution.

5. **Other Continuous Distributions**: Student's t, Exponential, Gamma, Chi-square Distributions.

We will also go over several important functions from the `stats` package including

-   "d" functions, which return a vector of **densities**.

-   "p" functions, which give us a cumulative **probability** of the distribution (also known as a distribution function).

-   "q" functions returns a probability corresponding to a given **quantile**.

-   and finally, "r" functions generate **random** values from a given distribution.

<!--chapter:end:index.Rmd-->

---
output:
    bookdown::html_document2: default
---
# Basic Concepts of ggplot {#ggplot}

A visualization can be anything. It can be a drawing, a photograph, a sculpture, a well-decorated cake... you get the idea. However, data scientists and statisticians are often not very skilled at all of these art forms, so they are limited to their tools at hand: functions, code, and geometries.

This leads us to data visualizations. What separates a data visualization from a drawing are the tools used to construct it. In this guide, our blank canvas is a function called `ggplot`.

```{r, empty-plot, message= FALSE, warning = FALSE, fig.cap="An empty plot", out.width="80%", fig.asp=.75, fig.align="center"}
library(tidyverse)
ggplot()
```

## Geometries and Aesthetic Mappings

Every time that we want to visualize something, we must use this function. The stuff that fills up this space are called **geometries**. Here are just a few that ggplot has available to us.

- `geom_rect` produces rectangles.
- `geom_point` creates dots.
- `geom_line` draws lines.
- `geom_bar` makes a barplot.
- `geom_function` uses functions to draw a continuous curve.

If the geometries are the shapes on our ggplot canvas, how do we put them on there? This is where aesthetics and mapping come in. When we call the function `ggplot` we need to connect our data to aesthetic mappings which are then applied to various geometries.

But where do we use aesthetics mappings? The `mapping` argument in the `ggplot` and `geom` functions are what connect variables from our dataset. Lets look at a dataset of Marvel superheroes and apply the following aesthetics:

```{r, avengers-formatting, include=FALSE}
library(fivethirtyeight)
data(avengers)

glimpse(avengers)

avengers <- avengers %>%
  select(name_alias, gender, appearances, years_since_joining, death1) %>% 
  arrange(desc(appearances)) %>%
  rename(death = death1) %>% 
  slice(1:20) 

avengers$gender = str_to_lower(avengers$gender)

avengers$gender[avengers$name_alias == "Susan Richards (nee Storm)"] <- "female"
```

Years since joining Marvel → x

Appearances → y 

Gender → color

Now we will use the `aes` function to define our mappings for a simple scatterplot. We will also use the `labs` function to label our axes and title and an additional `size` argument to make our points larger.

```{r, avengers-plot, fig.align="center", out.width="80%", fig.cap = "Basic Avengers scatterplot"}
ggplot(data = avengers, mapping = aes(x = years_since_joining,
                                      y = appearances,
                                      color = gender)) +
  geom_point(size = 5) +
  labs(title = "'Age' of Superheroes and Appearances",
       x = "Years since Joining Marvel",
       y = "Appearances")
```

We used two arguments in the ggplot function: data and mapping. Next, we added the geometry `geom_point` using a plus sign (+). The aesthetic mapping in the ggplot function are then passed onto `geom_point`.

Even though we must include data and mappings to produce a visualization, we do not always have to name the arguments themselves. For instance, we could simply write:

```{r, avengers-simple, eval=F, results = "hide"}
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances,
                     color = gender)) +
  geom_point()
```

Aesthetic mappings can also produce different geometries by simply changing the function. For example, instead of using `geom_point` we can use `geom_rect` to draw rectangles or squares.

```{r, avengersplot-rect, fig.align="center", out.width="80%", fig.cap="Avengers scatterplot with rectangles"}
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances,
                     fill = gender)) +
  geom_rect(aes(xmin = years_since_joining - 1,
                xmax = years_since_joining + 1,
                ymin = appearances - 100,
                ymax = appearances + 100)) +
    labs(title = "'Age' of Superheroes and Appearances",
       x = "Years since Joining Marvel",
       y = "Appearances")
```

Notice in Figure \@ref(fig:avengersplot-rect) that the `geom_rect` geometry requires aesthetic mappings beyond those in `geom_point`, including the minimums and maximums for x and y. The color aesthetic of `geom_point` turns to fill in `geom_rect` since the rectangles are not points; they are shapes with empty space.

So far we have established that aesthetics can be passed onto different geometries. 

Figure \@ref(fig:ggplot-diagram) recaps where we are at now.

```{r, ggplot-diagram, results="markup", echo=FALSE, fig.align="center", out.width="80%", fig.cap="ggplot diagram"}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, fillcolor = Linen]

data [label = 'Data']
ggplot [label = 'ggplot']
map [label =  'Mappings \n Aesthetics']
geometries [label = 'Geometries']
plot [label= 'Plot']

# edge definitions with the node IDs
{data map}  -> ggplot -> geometries -> plot
}")
```

Generally, we want to provide mappings to the ggplot function so that they pass to all of our geometries. Including them in every line of code would be repetitive.

```{r, avengers-aes, eval=F, results = "hide"}
# Don't do this!
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances,
                     color = gender)) +
  geom_point(aes(x = years_since_joining,
                 y = appearances,
                 color = gender))

# Do this instead!
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances,
                     color = gender)) +
  geom_point()
```

But there are plenty of instances in which we wouldn't want to keep our aesthetics limited to the ggplot function line--for example, plots with two different groups or those with different scales on the y axis (although it is not generally recommended to have multiple scales on an axis since it is confusing). Let's return to the Avengers plot but keep the `color` argument in `geom_point` so that the color of our text doesn't differ by gender.

```{r, avengersplot-gender, fig.align="center", out.width="80%", fig.cap = "Avengers scatterplot with unique color and label aesthetic assignments"}
ggplot(avengers, aes(x = years_since_joining, y = appearances)) +
  geom_point(aes(color = gender), size = 5) +
  geom_text(aes(label = name_alias), size = 3) +
  labs(title = "'Age' of Superheroes and Appearances",
       x = "Years since Joining Marvel",
       y = "Appearances")
```

Figure \@ref(fig:avengersplot-gender) doesn't look pretty but you get the idea. A general rule to follow with data visualizations is to avoid mapping one aesthetic to multiple variables. For instance, a plot in which the color is used to illustrate both an Avenger's gender and their name would be confusing.

```{r, avengersplot-wrong, fig.align="center", out.width="80%", fig.cap= "Incorrect Aesthetic Mapping"}
# Don't do this!
ggplot(avengers, aes(x = years_since_joining, y = appearances)) +
  geom_point(aes(color = gender), size = 5) +
  geom_text(aes(color = name_alias, label = name_alias), size = 3) +
  labs(title = "'Age' of Superheroes and Appearances",
       x = "Years since Joining Marvel",
       y = "Appearances")
```

Sadly, Steve Rogers is not a gender and neither are the other Avenger names in the legend. This is why we should not map one aesthetic to more than one variable.

## Altering Aesthetics and Scalings 

Now, let's introduce a way to change our aesthetics further. There are some arguments within geom functions that allow us to do this. For instance, you might have noticed that I used `size` to make points and labels larger in the previous plots. We can also use `color` or `fill` to make adjustments to our geoms.

```{r, avengersplot-red, fig.align="center", out.width="80%", fig.cap= "Avengers scatterplot in red!"}
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances)) +
  geom_point(color = "tomato", size = 5)
```

But previously we have related color with an Avenger's gender. ggplot uses some default colors to differentiate male and female Avengers but we can change this further with **scales**. Every scale function has three components:

1. `scale_`
2. The name of the aesthetic we are scaling. In this case, `color`.
3. A method of applying the scale. In this case, we'll use `manual`.

So to combine all of the previous steps together, we would call the function`scale_color_manual` to directly alter the colors of our aesthetic related to gender.

```{r, avengersplot-scale, fig.align="center", out.width="80%", fig.cap = "Avengers scatterplot using `scale_color_manual`"}
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances,
                     color = gender)) +
  geom_point(size = 5) +
  scale_color_manual(values = c("Tomato", "Navy"))
```

We've not only learned about the gender gap in the Marvel universe but also how to apply scales to our aesthetics! There are a number of scales that we can use further. For example, `scale_alpha_binned` allows us to alter the transparency of points after providing a `range` between 0-1. The `breaks` argument separates our observations into different groups.

```{r, avengersplot-alpha, fig.align="center", out.width="80%", fig.cap = "Avengers scatterplot with `scale_alpha_continuous`"}
ggplot(avengers, aes(x = years_since_joining,
                     y = appearances,
                     alpha = appearances)) +
  geom_point(size = 5) +
  scale_alpha_continuous(range = c(0.2, 1),
                         breaks = c(2000, 3000, 4000))
```

An important fact to remember is that mapping a variable to more than one aesthetic sometimes exaggerate differences in data. In this case, using two aesthetics--y and alpha--for appearance count draws the eye to the Avengers with the most appearances. This is why any data scientist must consider the [principles of data visualization](https://socviz.co/index.html#preface) and how certain techniques mislead or distort data.

So far, we have learned how to call the `ggplot` function, apply aesthetic mappings, layer geometries, and alter aesthetics with scales. Any data visualization, regardless of its complexity, follows this formula. 

Hopefully this chapter has provided you a some more confidence as you begin producing your own visualizations. However, this is by no means an exhaustive tutorial. To learn more about data visualizations here are some useful resources:

- [R Cookbook](https://www.amazon.com/gp/product/0596809158/ref=as_li_tf_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0596809158&linkCode=as2&tag=cooforr09-20)
- [Cookbook for R](http://www.cookbook-r.com/)
- [The R Graph Gallery](https://www.r-graph-gallery.com/index.html) 
- [R for Data Science Online Community](https://www.rfordatasci.com/)

The rest of this guide specifically uses our new visualization tool to learn about the statistical distributions that govern our world. Click on to learn more!

<!--chapter:end:01-ggplot.Rmd-->

---
output:
    bookdown::html_document2: default
---
# Understanding Distributions {#Distributions}

```{r, library-theme, include=FALSE}
library(tidyverse)
theme_set(theme_light())
```

Every distribution discovered in the probability world is determined by a **random variable**. These variables are the not the same ones that you were exposed to in algebra or calculus class. Instead, they are used to map random processes, like rolling dice or playing the lottery, to readable notation.

We'll begin with a simple example of this by flipping a coin. Let the random variable X (usually random variables are capital letters) denote this process. It can take two forms, heads or tails. We can use a binary 0 or 1 to show this:

$$X = \{0,1\}$$

You might be asking yourself: "What does this have to do with probability?" We begin by defining the random variable so that we can later discover its mathematical attributes, such as its mean or variance, for example.

We can also define the probability that the random variable $X$ (the coin) takes the form of heads or tails by using probability notation.

$$P(X = x)$$

Where x can equal 0 or 1. We can interpret the statement above as "the probability that the random variable X equals x." So what is that probability? By common sense, we can conclude that the probability that you get heads is $P(X=1)=1/2$. We can extend this logic to show that **the sum of probabilities for any random variable is 1**.

$$P(X = 0) + P(X = 1)= 1/2 + 1/2 = 1$$

In other, more complicated probability distributions, we use random variables such as $X$ to mark these processes without having to explain them each time.

## Uniform Distribution {#Uniform}

We'll begin our journey with uniform distribution. Given $a < b$, we can define this distribution's **density** as:

$$ P(X=x) = \frac{1}{b-a}\quad \textrm{for} \enspace a \leq x \leq b $$

When we refer to densities we are talking about probabilities. The two terms are interchangeable, but the term **density** emphasizes the finite probability space that a distribution occupies. As mentioned previously, the sum of probabilities for any distribution is 1.

How do we get this equation? Imagine drawing a rectangle in the interval (a,b) with height 1/(b-a) as in Figure \@ref(fig:uniform-basic).

```{r, uniform-basic, fig.cap= "Remember Scales? See if you can understand the code", fig.align="center", out.width="80%"}
ggplot() +
  geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), 
            fill = NA,
            color = "black") +
  labs(x = "x", y = "Density") +
  scale_x_continuous(breaks = c(0,1), labels = c("a", "b")) +
  scale_y_continuous(breaks = c(0,1), labels = c("", "h"))
```

We have the height as $\frac{1}{(b-a)}$. We know this since the cumulative area of a distribution must be 1 and the width is b-a. Using the area of a rectangle with h being the height of the uniform distribution.

$$(b-a)h=1$$

For the equation to hold, h must equal 1/(b-a).

Let's define X as a **discrete** random variable that is uniform on 1 through 10. The term "discrete" means that $X$ can only be integers, and the fact that its probabilities are uniform means that every integer has an equal probability of occurring. Therefore:

$$
\begin{split}
&X=\{1,2,...,10\}\\&P(X=x)=\frac{1}{10-1}=1/9\quad\textrm{for}\quad1\leq x\leq 10 
\end{split}
$$

The first line outlines the random variable X and the numbers that it can take. The second line then calculates the probability of the random variable taking the integers 1-10 as 1/9. In other words, the **density**, or individual probability, for each of $1\leq x\leq 10$ is 1/9.

The first `stats` function we can use with the uniform distribution is `dunif` which also returns the density at point x in the distribution given left and right bounds. This is quite easy to determine in the uniform distribution because **every point has the same density of h**- in other words, the height. `dunif` has 3 arguments.

1. A vector of values to calculate densities for.

2. The minimum or `min` of the distribution.

3. The maximum or `max` of the distribution.

We'll return to our example where the beginning and end of the uniform distribution is at x = 1 and x = 10, respectively. What are the densities at x = 1, 5, and 10?

```{r, uniform-densities}
dunif(c(1, 5, 10), min = 1, max = 10)
```

You are correct if you guessed 1/9 or approximately 0.111! If $a = 1$, $b = 10$ then by definition $h = 1/9$. Thus, as we calculated earlier, every point in the distribution has a density of 1/9.

We can visualize this by drawing a rectangle using `geom_rect` and then adding points with `geom_point`. Remember that certain geometries require aesthetics beyond x and y. 

```{r, geomrect-plot, fig.cap= "Uniform distribution with marked densities", fig.align="center", out.width="80%"}
ggplot() +
  geom_rect(aes(xmin = 1, xmax = 10, ymin = 0, ymax = 1), fill = NA, 
            color = "black") +
  labs(x = "x", y = "Density") +
  geom_point(aes(x = c(1, 5, 10), y = 1), size = 5, color = "navy")
```

The `punif()` function helps us find the cumulative density between at the qth quantile. Using a different rectangle with $a = 0$, $b = 4$, and $h = 1/4$, we can find the proportion of the distribution between the minimum of 0 and maximum of 2 by writing the following:

```{r, punif}
punif(2, 0, 4, lower.tail = TRUE) # Remember: we don't always have to name our arguments if we know the order.
```

We can similarly write:

```{r, punif-alternative}
sum(dunif(0:1, min = 0, max = 4)) # Not including 2.
```

There are three arguments in this function: a vector of quantiles, a minimum, maximum, and a binary `lower.tail` argument. By default this is set to `TRUE`.

We can also think about the `punif()` function as drawing a smaller rectangle from 0 to 2 (if `lower.tail = FALSE`) and calculating its area. Below, this is the same as the percent of the total area that the navy rectangle occupies.

```{r, punif-plot, fig.cap= "Uniform distribution with cumulative density marked", fig.align="center", out.width="80%"}
ggplot() +
  geom_rect(aes(xmin = 0, xmax = 2, ymin = 0, ymax = 1/4),  # Small rectangle
            alpha = .2, 
            fill = "navy", 
            color = "navy", 
            linetype = "dashed") +
    geom_rect(aes(xmin = 0, xmax = 4, ymin = 0, ymax = 1/4), # Large rectangle
              fill = NA, 
              color = "black") +
  labs(x = "x", y = "Density")
```

$$ area \hspace{0.2 cm}= \hspace{0.2 cm}base \hspace{0.2cm} * \hspace{0.2 cm}height$$

$$area = 2*0.25=0.5$$

Now we will consider the opposite scenario where we want to find the x-value that corresponds with the 50th percentile of our distribution. This is the purpose of `qunif()`. We already know from using the distribution function `punif()` that this is 2. 

```{r, punif-opposite}
qunif(0.5, 0, 4) #lower.tail is also set to TRUE by default
```

The last function `runif()` generates random deviates within the distribution. There are three arguments in this function:

- n, the number of deviates we want to produce

- min, the left bound of the uniform distribution

- max, the right bound of the uniform distribution

We'll use the `round()` function to round them to the hundredths place.

```{r, uniform-deviates, fig.cap= "Uniform distribution with random deviates", fig.align="center", out.width="80%"}
# Create object of deviates

unif_dev <- round(runif(10, min = 0, max = 4), digits = 2)

# Plot

ggplot() +
  geom_rect(aes(xmin = 0, xmax = 4, ymin = 0, ymax = 1/4), 
            fill = NA,
            color = "black") +
  geom_point(aes(x = unif_dev, y = 1/4), size = 5, color = "navy") + # Plot deviates
  labs(x = "x", y = "Density") +
  scale_x_continuous(breaks = seq(0, 4, by = 1), labels = seq(0, 4, by = 1))
```

The fact that the deviates appeared on non-integer values (i.e. those that would normally be represented by fractions or decimals) reveals that the uniform distribution is not always discrete. In this case, as well as others later in this guide, distributions are labeled **continuous**. As we investigate several other types of distributions, we'll start with the discrete cases and then transition to the continuous ones.

<!--chapter:end:02-uniform.Rmd-->

---
output:
    bookdown::html_document2: default
---
# Other Discrete Distributions {#Discrete}

## Geometric Distribution {#Geometric}

```{r, include=FALSE}
library(tidyverse)
library(cowplot)
theme_set(theme_light())
set.seed(0)
```

For the next distribution, imagine a basketball player that is not particularly good. Whenever he takes a free throw, there is a 10% probability that he makes it. How many free throws can we expect him to shoot before he makes one?

The geometric distribution helps us answer this question. We can think of this as a spread of $N$ trials required to reach a probability $p$ in the following equation.

$$P(N = n) = (1-p)^{n} p \hspace{0.7cm}for\hspace{0.2cm} n=0,1,...,\infty$$

Since trials can only be measured in integers, the geometric distribution is another **discrete** distribution.

Although there is a more in-depth proof, the formula for this distribution is already quite intuitive. First, the geometric distribution needs $n$ failures and one success for $N to equal n$. Likewise, if the probability of a success is represented by $p$, the probability of a failure must be $1-p$.

When can we use a geometric distribution, and does it apply to the free throw example? In order to model a random process with the geometric distribution, it must meet the following assumptions:

1. Every "trial" must be independent of each other. In other words, the outcome of one trial does not have any bearing on other outcomes.

2. There are only "successes" and "failures." If a response has more than two outcomes, we should not use the geometric distribution.

3. The probability of "success" remains consistent for each trial.

It is possible that these assumptions may not be fully met in our case study. For example, does the basketball player get tired over time, which would decrease their probability of a successful free throw? This would violate the third assumption. 

The geometric distribution has a mean or expected value of $\frac{1-p}{p}$. This means that if we had many similar basketball players shoot free throws, their long run average "failures" would be ($\frac{1-0.1}{0.1}=9$), i.e. their average "successful" free throw would be on shot 10. 

-----------------------------------------------------------------------

Now that we have found the long run mean of the geometric distribution, let's see how we can visualize the first thirty shots that the basketball player takes. Again, we'll use a d or density function (in this case called `dgeom`) to produce the individual densities for each of the shots in the total distribution. `dgeom` has 3 arguments.

1. A vector of integers in order to return their corresponding densities. We can imagine this as the probability of $x$ failures before the first success. Here, we'll use `0:30` to represent the integers 0, 1, 2, ... , 29, 30.

2. The `prob` or probability of a successful free throw.

3. An optional `log` argument.

```{r, geom-plot, fig.align="center", out.width="80%", fig.cap = "Histogram of basketball player's shot probabilities"}
# Create a data frame with the first 30 values, incremented by 1, as well as a geometric distribution.
dgeom_df <- tibble(failures = 0:30,
                   density = dgeom(x = 0:30, prob = 0.1, log = FALSE))

# Create plot
ggplot(dgeom_df) +
  geom_bar(aes(x = failures, y = density), stat = "identity") +
  labs(x = "Missed Shots", y = "Density")
```

Each ith value on the x axis and its corresponding jth y value can be conceptualized as "There is a j probability that the basketball player had i failures before his first successful free throw."

As suggested in the plot above, the probability that a shot is the first success approaches 0 as the observation number approaches infinity.

Now let's talk about the `stat` argument in `geom_bar`. By default, this is set to `"count"`. This means that the height of an x-value's bar is determined by the number of times that it appears in a dataset. By changing this argument to `"identity`", ggplot looks for a corresponding y aesthetic to determine the height of the bar.

If we don't alter the behavior of `geom_bar` we would get a plot that looks like this:

```{r, geomplot-incorrect, fig.align="center", out.width="80%", fig.cap= "Incorrect geom_bar plot"}
# Don't do this!
ggplot(dgeom_df) +
  geom_bar(aes(x = failures)) +
  labs(x = "Missed Shots", y = "Density")
```

------------------------------------------------------------------------

To calculate the probability of a range of outcomes we can use the function `pgeom`. For example, how likely is it that the player can make a free throw in 9 or fewer shots? To find this, we'll plug in $q = 8$.

```{r, pgeom}
# Create a cumulative density function
pgeom(8, 0.1)
```

We plug in $q=8$ instead of $q=9$ because the first argument of function requires the number of failures until the first success. In other words, the value that `pgeom` returns can be thought of as the probability that we have less than or equal to 8 failures before the first successful free throw.

When using a "p" or distribution function with `lower.tail = TRUE`, the first argument is our right bound. If we change to `lower.tail = FALSE`, the first argument becomes our left bound.

We can think of our calculation above as the same as finding the area of the binomial distribution from 0 to 8--i.e. the total area of the first nine rectangles. We will fill the first nine bars blue and the rest gray.

```{r, pgeom-plot, fig.align="center", out.width="80%", fig.cap= "Basketball plot highlighting area of first 9 failures"}
ggplot(dgeom_df) + # Insert the vector of densities into the data argument
  geom_bar(aes(x = failures, y = density),
           stat = "identity",
           fill = ifelse(dgeom_df$failures <= 8, "skyblue1", "grey35")) +
  labs(x = "Missed Shots", y = "Density")
```

Here are a few things to remember from the code above before we move on.

-The `ifelse` function is in base R and allows us to set a condition for the fill aesthetic of our bar plot. We are essentially telling ggplot to fill each bar blue if the `shot_num` is less than or equal to 9. Otherwise, fill the rest of the bars gray.

-By default, the `lower.tail` argument is set to `TRUE`, meaning that we are finding the area of the distribution from the first shot to the ninth shot. However, if we set the `lower.tail` argument to `FALSE`, we will get the area of the distribution after the eighth shot.

```{r, pgeom-uppertail}
pgeom(8, prob = 0.1, lower.tail = FALSE)
```

```{r, pgeomupper-plot, fig.align="center", out.width="80%", fig.cap= "Basketball plot highlighting area after 8th failed shot"}
# As represented on a bar plot
ggplot(dgeom_df) +
  geom_bar(aes(x = failures, y = density),
           stat = "identity",
           fill = ifelse(dgeom_df$failures > 8, "skyblue1", "grey35")) +
  labs(x = "Missed Shots", y = "Density")
```

------------------------------------------------------------------------

The `qgeom` function can be thought of as the inverse of `pgeom`. Because of this, we enter in a cumulative density (value from 0-1) into the function and it returns the shot number in which we would expect to reach that cumulative density, from left to right on a number line.

Like `pgeom`, keep in mind that `lower.tail` is set to `TRUE` by default.

By what shot can we begin expecting the player have already made a free throw? To answer this question, we will enter 50% as a cumulative density.

```{r, qgeom}
qgeom(0.5, prob = 0.1)
```

```{r, qgeom-plot, fig.align="center", out.width="80%", fig.cap= "Basketball plot highlighting lower 50% of distribution"}
dgeom_df <- dgeom_df %>%
  mutate(cume_dist = cumsum(density))

ggplot(dgeom_df) +
  geom_bar(aes(x = failures, y = density),
           stat = "identity",
           fill = ifelse(dgeom_df$cume_dist <=.5, "skyblue1", "grey35")) +
  labs(x = "Missed Shots", y = "Density")
```

There is quite a bit to unpack here. We use `mutate` to create a new variable for the cumulative density at each shot. This is the sum of a shot's density and all of the previous shots before it.

Next, we return to the `ifelse` function to determine which bars to fill. We can interpret our code as "If the cumulative density of this shot is less than or equal to .5, fill it blue; otherwise, fill it gray."

------------------------------------------------------------------------

Finally, imagine that 1,000 exact copies of the basketball player shoot free throws. Below is the distribution of those outcomes. The function `rgeom` simulates each of the 1,000 players and returns the number of their first successful shot.

```{r, rgeom-plot, fig.align="center", out.width="80%", fig.cap="Plot of 1,000 random geometric deviates"}
rand_geom <- tibble(deviates = rgeom(1000, 0.1))

ggplot(rand_geom) +
  geom_histogram(aes(x = deviates),
                 binwidth = 2) +
  labs(x = "First Shot", y = "Frequency") +
  scale_x_continuous(breaks = seq(0, 80, 10)) # Added more ticks on x axis
```

As stated at the beginning of this chapter, the geometric distribution can be thought of the spread of N trials required to get probability p. Because of this, as we increase the number of simulated basketball players, we reach a distribution more and more similar to the actual binomial distribution.

To visualize this, we'll generate four plots: three randomly-sampled geometric distributions and a "true" geometric distribution. To simplify our code, we'll start by writing a new function called `geom_plot`. If function-writing is unfamiliar to you, I encourage you to read this [friendly introduction](https://r4ds.had.co.nz/functions.html).

```{r, sample-plot, fig.align="center", out.width="80%"}
# Create a function that randomly samples from geometric distribution with sample size n
geom_plot <- function(prob, n){
  temp <- tibble(deviates = rgeom(n, prob))
  plot <- ggplot(temp, aes(x = deviates)) + 
    geom_histogram(binwidth = 2) +
    scale_x_continuous(breaks = seq(0, 80, 20)) +
    labs(x = NULL, y = NULL, caption = str_c(n, "simulations", sep = " "))
  return(plot)
}
 
# Generate random samples and plots
sample_1000 <- geom_plot(0.1, 1000) +
  labs(y = "Count",
       title = "Random Deviates")
sample_10000 <- geom_plot(0.1, 10000)
sample_100000 <- geom_plot(0.1, 100000)


# Generate true geometric distribution 
dgeom <- tibble(shot_num = 1:80,
                density = dgeom(1:80, prob = 0.1, log = FALSE))

true_plot <- ggplot(dgeom) +
  geom_bar(aes(x = shot_num, y = density), stat = "identity", width = 0.5) +
  labs(x = "First Shot", 
       y = "Density", 
       title = "Geometric Distribution") +
  scale_x_continuous(breaks = seq(0, 80, 20))

# Plot all together in a grid
cowplot::plot_grid(NULL, true_plot, NULL, sample_1000, sample_10000, sample_100000, ncol = 3, nrow = 2)
```

## Binomial Distribution {#Binomial}

While we would use the geometric distribution to model the number of failed free throws until a successful shot, the **binomial distribution** instead considers the probability of successful free throws in a fixed number of attempts. It is defined below:

$$P(K = k) = C_{n,k}p^{k}(1-p)^{n-k}\\\textrm{where} \hspace{0.2cm}C_{n,k}\hspace{0.2cm}\textrm{is the number of ways that we can pick k successes in n trials.}$$

At first glance, this formula is complicated, but it can be easily divided into two parts:

-   $C_{k,n}$ represents the number of ways we can select k successes in n trials. This combination can be calculated as $C_{n,k}=\frac{n!}{k!(n-k)!}$. 

-   $p^{k}(1-p)^{n-k}$ or the probability of getting k successes and therefore n-k failures.

We multiply the scenarios term by the probability term to account for the fact that there are multiple ways that we can get to the kth success. A basketball player does not need to make k shots in a particular order.

For example, if we are interested in the probability that the basketball player shoots 30 free throws and makes 3 of them, we can plug these numbers into our equation.

-   We have n = 30 independent trials, assuming that the player shoots at the same place and does not get tired.

-   We want to have k = 3 successful free throws and n- k = 27 unsuccessful ones.

-   Recall that the probability of a successful shot is p = 10%. Thus, a missed shot has probability 1 - p = 90%.

Thus, our density formula allows us to find the probability of a basketball player making 3 shots using simple multiplication.

$$P(K = 3) = C_{30,3}(.10)^{3}(1-.10)^{27}\approx.2361$$

With R, the function `dbinom` similarly accomplishes this.

```{r, dbinom}
# The arguments do not all need to be named, but I will do so here.
dbinom(x = 3, size = 30, prob = .1)
```

In other words, the probability that the basketball player shoots 30 free throws and makes **exactly** 3 of them is about 0.2361.

As we begin visualizing this with `ggplot`, we should return to our strategy from the geometric chapter by creating a data frame or tibble with two columns--one to signify all of the possible number of successes that we can have, and another of their corresponding probabilities or densities.

```{r, binom-df}
dbinom_df <- tibble(shots = 0:30,
                 density = dbinom(0:30, size = 30, prob = .1))
```

We can now plot this with `geom_bar`.

```{r, binom-plot}
ggplot(dbinom_df) +
  geom_bar(aes(x = shots, y = density),
           stat = "identity") +
  labs(x = "Successful Shots", y = "Density") +
  scale_x_continuous(breaks = seq(0, 30, 2))
```

As seen above, it is quite unlikely that the basketball player will make more than 10 shots. In fact, their probabilities aren't even large enough for the bars to register on the plot.

------------------------------------------------------------------------

In some scenarios in the statistics world, we aren't only interested in the probability of getting **exactly** k successes; we may want to consider a range of outcomes. For example, a personal coach who records the basketball player's progress wants to determine the probability that he makes 3 **or fewer** shots out of 30.

Using the previous chapters, we can assume that we would want to use the distribution function `pbinom` to answer this question. 

```{r, pbinom}
# Before you continue, think about what the four arguments mean. Why would we set the lower.tail to TRUE instead of FALSE?
pbinom(3, 30, 0.1, lower.tail = TRUE)
```

The first argument in the above function is similar to the other "p" functions since it can be thought of as the "right bound" of interest. We can imagine the `pbinom` function as finding the area of the binomial distribution from 0 to 3.

```{r, pbinom-plot}
ggplot(dbinom_df) +
  geom_bar(aes(x = shots, y = density),
           stat = "identity",
           fill = ifelse(dbinom_df$shots <= 3, "purple2", "lightgray")) +
  labs(x = "Successful Shots", y = "Density")
```

Just as we used it in the geometric distribution section, the `ifelse` function allows us to fill our bars with different colors depending on the "shot" value. The interpretation here is "fill the bar purple if number of successful shots is three or fewer. If not, fill the bar gray."

------------------------------------------------------------------------

Like the other "r" functions,`rbinom` randomly simulates the outcomes of a distribution. In this case, the function returns the number of successful free throws made all of the simulations. This is useful because it helps us better understand the natural random process.

```{r, rbinom}
ranbinom <- tibble(deviates = rbinom(1000, 30, .10))

glimpse(ranbinom)

ggplot(ranbinom) +
  geom_bar(aes(x = deviates), stat = "count") +
  scale_x_continuous(breaks = seq(0,10, 1)) +
  labs(x = "Successful Shots", y = "Frequency")
```

***

## Negative Binomial Distribution

Next, we'll introduce the **negative binomial distribution**. Similar to the from the geometric distribution, we are interested in modeling the number of failures in a binary process. However, we are not only interested in how long it takes for one success; we are interested in modeling the number of failures until a particular number ($r$) of successes.

$$\begin{equation}
P(Y=y) = \binom{y + r - 1}{r-1} (1-p)^{y}(p)^r \quad \textrm{for}\quad y = 0, 1, \ldots, \infty.
\end{equation}$$

An interesting case appears when we set $r=1$, i.e. the number of failures until one success.

$$\begin{split}
P(Y=1) &= \binom{y + 1 - 1}{1-1} (1-p)^{y} (p)^1 \\ &= \binom{y}{0} (1-p)^yp \\ &=(1-p)^yp \quad \textrm{for} \quad y = 0, 1, \ldots, \infty
\end{split}$$

Notice how the last line of this looks exactly like the **geometric distribution**? This is because the geometric distribution is a special case of the negative binomial distribution.

Now let's return to the basketball example. As we have established before, there is about a 4% chance that our player makes 8 failures before his first successful shot. What is the probability that he makes 8 failures before his second successful shot?

```{r, dnbinom-plot}
dnbinom(8, 2, 0.1)

dnbinom_df <- tibble(failures = 0:30,
                  density = dnbinom(0:30, 2, 0.1))

ggplot(dnbinom_df, aes(x = failures, y = density)) +
  geom_bar(stat = "identity", fill = "turquoise1", color = "black") +
  labs(x = "Failed Shots", y = "Density", 
       title = "Number of Failures until Second Free Throw")
```

Now that we have a new parameter r, how does the distribution change as it increases or decreases?

```{r, dnbinom-mult}
nbinom_plot <- function(x, size, prob){
  temp <- tibble(failures = x,
                 density = dnbinom(x, size, prob),
                 cume_density = cumsum(density))
  median <- temp %>% filter(between(cume_density, .5, .55))
  plot <- ggplot(temp, aes(x = failures, y = density)) +
    geom_density(stat = "identity") +
    geom_vline(xintercept = median$failures[1]) +
    labs(x = NULL, y = NULL)
  return(plot)
}

p1 <- nbinom_plot(0:100, 3, 0.1) +
  ggtitle("r = 3, prob = 0.1")
p2 <- nbinom_plot(0:100, 5, 0.1) +
  ggtitle("r = 5, prob = 0.1") +
  labs(x = "Failures", y = "Density")
p3 <- nbinom_plot(0:100, 7, 0.1) +
  ggtitle("r = 7, prob = 0.1")
p4 <- nbinom_plot(0:100, 5, 0.15) +
  ggtitle("r = 5, prob = 0.15")
p5 <- nbinom_plot(0:100, 5, 0.08) +
  ggtitle("r = 5, prob = 0.05")

plot_grid(p1, NULL, p4, NULL, p2, NULL, p3, NULL, p5, ncol = 3, nrow = 3)
```

As you can see above, the center of the distribution moves right as $r$ increases or the probability of a success decreases.

## Poisson Distribution {#Poisson}

Unlike those based on Bernoulli processes (binary and independent outcomes), a **Poisson** process considers the number of times an event occurs in a given time or space. If $K$, for instance, is the number of events in an interval, we'll model it with the **Poisson Distribution**.

$$
P(K=k) = \frac{e^{-\lambda}\lambda^k}{k!} \quad \textrm{for} \quad \mathrm{k} = 0, 1, \ldots, \infty \\
\textrm{Where} \enspace \lambda \enspace \textrm{is the average number of events in a unit of time}
$$

Now, we'll put this formula into practice.

Consider a pet shelter which has an average of 10 adoptions per month. Every month, they rescue 9 pets. If the shelter currently has 40 pets, what is the probability that have less than 45 pets at the end of the month in order to prevent overcrowding?

From above, we can deduce that the shelter would need to have *more than* 4 adoptions in order to prevent overcrowding. 

$$ \begin{split} \textrm{Current pets} \enspace + \enspace \textrm{Rescues} \enspace - \enspace \textrm{Adoptions} &= \textrm{Total pets} \\ 40 + 9 - \textrm{Adoptions} &<45 \\ \textrm{Adoptions} &> 4 \end{split} $$

Plugging in $\lambda = 10$ and $k=5,6,7,\ldots40$

$$ \begin{split}
P(K=5) + P(K=6) \ldots + P(K=40) &= \frac{e^{-10}10^5}{5!} &+ \frac{e^{-10}10^6}{6!} &+ \ldots + \frac{e^{-10}10^{40}}{40!} \\ &=\frac{2500}{3e^{10}} &+ \frac{12500}{9e^{10}} &+ \ldots + \frac{10^{40}}{e^{10}40!} \\ &= 0.03783 &+ 0.06305 &+ \ldots +0 \\ &\approx 0.97075 \end{split}$$

There is about a 97% probability that the pet rescue center has at least 5 adoptions. In other words, it's quite likely that they will not have overcrowding.

Using R, we can use the density function `dpois` and more easily calculate the densities from 5 to 40 adoptions.

```{r, sum-dpois}
sum(dpois(5:40, lambda = 10))
```

The `ppois` distribution function also work here.

```{r, ppois}
ppois(4, 10, lower.tail = FALSE) # 4 is not included
```

```{r, adoptions-plot}
adoptions_df <- tibble(num_adopt = 0:40,
                    density = dpois(0:40, 10))

ggplot(adoptions_df, aes(x = num_adopt, y = density)) +
  geom_bar(width = 0.5, stat = "identity", 
           fill = ifelse(between(adoptions_df$num_adopt, 5, 40), "orchid1", "black")) +
  labs(x = "Number of Adoptions per Month",
       y = "Density")
```

As we have established, the probability mass function for a poisson distribution has a mean of $E(K) = \lambda$. The above plot seems to agree with this, since the distribution is roughly symmetric about 10.

The **standard deviation** of a poisson distribution is $\sqrt{\lambda}$. In this case, one standard deviation would be $\sqrt{10}\approx3.16$.

How does the distribution change as we manipulate our parameter $\lambda$? Applied to our example, how do the overall spread of adoptions change as the average number of adoptions change per month?


```{r, pois-plot}
pois_plot <- function(x, lambda){
  temp <- tibble(num_adopt = x,
                 density = dpois(x, lambda),
                 cume_density = cumsum(density))
  plot <- ggplot(temp, aes(x = num_adopt, y = density)) +
    geom_bar(stat = "identity", width = 0.2) +
    labs(x = NULL, y = NULL)
  return(plot)
}

adoptplot_1 <- pois_plot(0:40, 2) +
  ggtitle("Lambda = 2")
adoptplot_2 <- pois_plot(0:40, 3) +
  ggtitle("Lambda = 3")
adoptplot_3 <- pois_plot(0:40, 5) +
  ggtitle("Lambda = 5")
adoptplot_4 <- pois_plot(0:40, 10) +
  labs(title = "Lambda = 10", x = "Number of Adoptions", y = "Density")

plot_grid(NULL, adoptplot_4, NULL, adoptplot_1, adoptplot_2, adoptplot_3, nrow = 2, ncol = 3)
```

As you can see, the poisson distribution becomes more symmetric as $\lambda$ increases. When $\lambda$ is relatively small, the distribution is more right-skewed.

<!--chapter:end:03-other_discrete.Rmd-->

---
output:
    bookdown::html_document2: default
---
# From Discrete to Continuous Cases {#DiscreteContinuous}
```{r, discrete setup, include=FALSE}
library(tidyverse)
library(cowplot)
theme_set(theme_light())
set.seed(0)
```

## Normal Approximation and DeMoivre's Problem

In a way, the binomial distribution is the parent of the normal distribution. I'll explain.

Several centuries ago, mathematician Abraham DeMoivre was asked to solve a gambling problem requiring him to flip a coin many times and counts the total number of heads. After repeatedly playing the game, he found that his results resembled a unique bellcurve shape.

In this section, we will simulate his experiment:

- First, we will use R to create a coin. The heads side of our fair coin will be represented with a "H" and tails with a "T".

```{r, coin}
coin <- c("H","T")
```

- Next, we will use `sample` to simulate random coin flips. The first argument of the function is for the elements of our random sampling process. The second `size` is the number of times that we will flip the coin. Finally, we will set `replace` to `TRUE` so that we keep both sides of our coin after flipping. 

```{r, random-coins}
flips <- tibble(flip_num = 1:3600, 
                outcome = sample(coin, size = 3600, replace = TRUE))

head(flips, 5)
```

- Now, let's save the total number of heads.

```{r, save-heads}
heads <- flips %>% 
  filter(outcome == "H") %>% 
  count()

heads
```

- Our last step is to find a way to repeat this game of 3600 flips. We will do so by creating a function.

```{r, coin-function}
get_heads <- function() {
  fair_coin <- c("H", "T") # Repeating code from the previous lines.
  flip <- tibble(flip_num = 1:3600, 
                 outcome = sample(fair_coin, size = 3600, replace = TRUE))
  heads_count <- flip %>% filter(outcome == "H") %>% count()
  return(heads_count)
}
```

- Using this function, we will repeat the coin flip game 1000 times and count the number of heads using `replicate`. Read more about the function's documentation by typing `?replicate` into the console.

```{r, coin-game}
outcomes <- tibble(game_num = 1:1000,
                   heads = as.numeric(replicate(1000, get_heads(), simplify = TRUE))) 

head(outcomes, 5)
```

- What does this look like? We will first save a ggplot object called `demoivre_plot` and then make a histogram.
  
```{r, coinplot-hist, fig.cap= "Coin game simulation with geom_histogram", fig.align="center", out.width="60%"}
# Plotting our game outcomes

demoivre_plot <- ggplot(outcomes, aes(x = heads))
  
demoivre_plot +
  geom_histogram(bins = 25) +
  labs(x = "Heads", y = "Count")
```

You can start to see the bell shape taking form in Figure \@ref(fig:coinplot-hist). However, this is even easier to see with `geom_density()` which takes the same aesthetic mapping as `geom_histogram`.

```{r, coinplot-density, fig.cap= "Coin game simulation with geom_density", fig.align="center", out.width="60%"}
demoivre_plot +
  geom_density(alpha = 0.4, fill = "lightsteelblue") +
  labs(x = "Heads", y = "Density")
```

This unique bellcurve roughly follows the **normal distribution**. 

This distribution is useful beyond the coin flipping game. It is also resembles many everyday phenomena such as heights, IQ scores, salaries, and blood pressure. Because of this, when we know that an outcomes, people, or observations are independent and normally distributed, we can make *inferences* about the larger picture.

***

## Normal Distribution

Now let's return to the normal distribution. We can define it as follows:

$$f(x)=(2\pi)^{-1/2}e^{-x^2/2} \quad \textrm{for} \enspace -\infty < x < \infty$$

The fact that $x$ can take non-positive and non-integer values differentiates the normal distribution from [discrete distributions](#Discrete). These such distributions are considered **continuous**.

The density function `dnorm(x)` can be thought of as f(x). The first argument x contains a vector of numbers that will yield their density. Let's try this with $x = 1.5$ on a standard normal distribution. This means that the mean of the distribution is 0 and the standard deviation is 1. 

How likely is it that a point would be 1.5 standard deviations above the mean?

```{r, dnorm}
dnorm(1.5, mean = 0, sd = 1) #The mean and sd arguments are set to these values by default.
```

To interpret this density, simply think of 13% as the probability that an observation would be 1.5 standard deviations greater than a mean. 
  
There are many real world scenarios in which we could apply this. For instance, the probability of an American male having a height that is 1.5 standard deviations greater than average (i.e. 76 inches or 6 feet and 3 inches) is about 13%.

***

Oftentimes, we don't have a standardized distribution with 0 as the mean and 1 as the standard deviation. To demonstrate this, we'll return to our height example. The average height of men in the United States is 70 inches with 2 inches as the standard deviation. Thus, we must reevaluate the `mean` and `sd` arguments.

```{r, height-args}
normal <- list(mean = 70, sd = 2)
```

We can visualize this differently than discrete distributions. `stat_function` is a unique tool within ggplot that plots continuous curves. There are several required arguments:

1. a geometry such as "function", "point", "area," etc.,
2. a continuous function,
3. lower and upper limits on the x and/or x axis,
4. and a list of arguments for the function.

We listed our arguments for a normal curve above. After plotting this curve, we will add a vertical line at the mean using `geom_vline`. For this geometry, we are required to provide an x-intercept.

```{r, normal-plot, fig.cap= "Normal distribution of U.S. male heights", fig.align="center", out.width="60%"}
normal_plot <- ggplot() +
  stat_function(
    geom = "function",
    fun = dnorm,
    xlim = c(61, 79),
    args = normal # This is the list of arguments from above
  ) +
  geom_vline(xintercept = 70, color = "lightsteelblue", linetype = "dashed") +
  labs(x = "Height (inches)", y = "Density")

normal_plot
```

Unlike the geometric and binomial distributions, the normal distribution is symmetric about the mean. When a height is much greater or smaller than the average, it is less likely to occur.

***

Now that we have made our density plot, we can again capture an interval of values in the distribution with our "p" distribution function `pnorm()`. If the shortest Duke basketball player (in the 2020-21 season) is 6 feet or 72 inches tall, what percentage of the American male population is shorter than the basketball team?

```{r, round-pnorm}
round(pnorm(72, 70, 2), digits = 3) * 100 # Multiply by 100 to get a percentage
```

To understand what the lower 84.1% of the normal distribution looks like, we will create two geometries with `stat_function` in Figure \@ref(fig:pnorm-plot). The first will shade the entire distribution gray. On top of that, the second `stat_function` geometry fills the area of the distribution lower than 72 inches blue.

```{r, pnorm-plot, fig.cap= "U.S. male heights below 72 inches", fig.align="center", out.width="60%"}
ggplot() +
  stat_function(fun = dnorm, # Plot the entire distribution
                geom = "area",
                fill = "lightgray",
                color = "black",
                xlim = c(61, 79),
                args = normal) +
  stat_function(fun = dnorm, # Change the color of heights < 72 inches.
                geom = "area",
                fill = "steelblue",
                xlim = c(61, 72),
                args = normal
                ) +
  labs(x = "Height (inches)", y = "Density")
```

As the blue portion of the plot makes clear, a majority of American men have heights below 72 inches.

***

We use `rnorm` whenever we want to draw random samples from the normal distribution. On its own, the function helps us better understand the natural variation of real world processes that draw from normal distributions. If we repeat this sampling many times, we'll have a plot that more closely resembles the *true* distribution.

Let's apply this concept again to height. Imagine that you took a random sample of 50 US men and took their height.

```{r, sample-heights, fig.cap= "Heights of U.S. men, n = 50", fig.align="center", out.width="60%"}
# Take sample
sample_50 <- tibble(deviates = rnorm(50, 70, 2))

# Plot as a histogram
ggplot(sample_50, aes(x = deviates)) +
  geom_histogram(bins = 20)
```

If we took a larger sample, we would get something that looks more normally distributed.

```{r, large-sample, fig.cap= "Heights of U.S. men, n = 5000", fig.align="center", out.width="60%"}
sample_5000 <- tibble(deviates = rnorm(5000, 70, 2))

ggplot(sample_5000, aes(x = deviates)) +
  geom_histogram(bins = 20)
```

The plots below will more finely measure this progression as sample sizes increase:

```{r, samples-comp, fig.cap= "Comparison of samples to normal distribution", fig.align="center", out.width="60%"}
# Create a function that randomly samples from normal distribution
norm_plot <- function(mean, sd, n){
  temp <- tibble(deviates = rnorm(n, mean, sd))
  plot <- ggplot(temp, aes(x = deviates)) +
    geom_histogram(bins = 20)
  return(plot)
}

# Produce plots
sample_50 <- norm_plot(70, 2, 50)
sample_500 <- norm_plot(70, 2, 500)
sample_5000 <- norm_plot(70, 2, 5000)

# Plot together
cowplot::plot_grid(NULL, normal_plot, NULL, sample_50, sample_500, sample_5000, ncol = 3, nrow = 2)
```

Now that we have looked at the normal distribution, let's continue to look at other continuous random variables.

<!--chapter:end:04-from_discrete_continuous.Rmd-->

---
output:
    bookdown::html_document2: default
---
# Other Continuous Distributions {#Continuous}
```{r, warning = FALSE, message= FALSE, include = FALSE}
library(tidyverse)
library(cowplot)
library(ggimage)
theme_set(theme_light())
set.seed(0)
```

## t-Distribution {#T}

We often don't know the true variance (and thus the standard deviation) of normally-distributed populations. However, we can still make estimations and inferences by adding a new parameter $t$, the **degrees of freedom**. After doing this, we have created a t-distribution. This is also known as a student's t-distribution.

Simply put, degrees of freedom comes from the number of observations in our sample that are able to, or have the *freedom*, to vary given our constraints. Since the only parameter that limits the freedom of our sample is the mean, we have $t = n-1$ degrees of freedom in a t-distribution.

Let's say that you have a sample of 5 observations: 1, 3, 5, 7, and an unknown number $x_5$. If we know that that the mean of the data is $\bar{x}=5$, we can conclude that $x_5=9$ given the definition of means.

$$\begin{split} \bar{x} &= \frac{\sum_{1}^p x_i}{p} = \frac{x_1+x_2+\ldots+x_p}{p} \\
5 &= \frac{1+3+5+7+x_5}{5} \\
x_5 &= 9
\end{split}$$

In other words, if we impose the mean as a constraint on our data, all but 1 observation in our sample is able to vary. Since $n$ is the sample size, having larger samples means more degrees of freedom.

Regardless of the degrees of freedom a t-distribution has, it will look like an approximately-normal curve with greater variability than the normal distribution when plotted. This why some describe the distribution as having thick "tails."

```{r tplot, fig.align="center", out.width="80%", fig.cap = "t-distribution with 5 degrees of freedom"}
ggplot() +
  geom_function(fun = dt, args = list(df = 5), color = "darkred") +
  xlim(c(-3, 3)) +
  labs(y = "Density")
```

I don't recommend that you use ggplot without something in the data argument since it limits your ability to alter aesthetics. However, for the purpose of simply visualizing continuous curves like the t-distribution, we can use `geom_function`.

As you see above, we need to provide `geom_function` a function as the first argument. `dt` is the density function for the t-distribution. By specifying the limits on the x-axis with `xlim` to -3 and 3, we isolated the visualization to the most important part of the curve.

As mentioned earlier, a t-distribution gains more degrees of freedom as the sample size increases. If we plot several curves with different degrees of freedom next to each other, we would notice that t-distributions with larger degrees of freedom have narrower tails and look closer to a normal distribution. This is because the magnitude of the last non-independent observation's effect on the mean weakens as there are more observations.  

Why is this the case? Imagine that you are working for an polling firm and want to measure the public's attitudes towards a new policy. You use a [feeling thermometer](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj2i9-mgujxAhUJ-Z4KHQx3DYUQFnoECAMQAA&url=https%3A%2F%2Felectionstudies.org%2F2007anes_gallup_questioncomparisons%2F&usg=AOvVaw0Q8BKck04Z4eMBT0hWyUqj) to gauge each individual's favorability on a scale from 0 to 100, where numbers higher than 50 represent favorable views.

Unless we poll every member in the public, we do not know the true mean or variance of the population's views towards the policy. Thus, we introduce the sample mean ($\bar{x}$) as a constraint, since we assume that random samples would accurately approximate the average person's views, given that the sample is **large enough** to be representative.

However, we must also make several other assumptions before using the t-distribution. For one, the response of poll participants must be **normally distributed** (or near normal). 

As mentioned previously, the sample size must also be **large enough** to avoid randomly-produced error. Although there is no definitive cutoff, polls with larger samples are usually more accurate. The impact of each respondent's opinion on the result of the poll decreases, meaning that the variability of distribution decreases.

```{r}
# Remember to set seeds before random processes!
set.seed(0)

# Generate random samples with mean = 50, sd = 12. Larger samples produce better approximations

sample_error <- function(true_mean, simulations, n) {
  mean(abs(true_mean - replicate(simulations, mean(rnorm(n, 50, 12)))))
}

tibble(`n = 2` = sample_error(50, 1000, 2),
       `25` = sample_error(50, 1000, 25),
       `250` = sample_error(50, 1000, 250),
) %>% knitr::kable(digits = 3, caption = "Average Error of Sample Means after 1000 Simulations")

x <- as_vector(c(2, 25, 250))

sample_error(50, 1000, c(2, 25, 250))
```

The change in variability as sample sizes increase is what gives some t-distributions thinner tails. Below, we'll plot several different t-distributions with various degrees of freedom as well as a normal distribution.

```{r, tplots, fig.align="center", out.width="80%", fig.cap = "t-distributions and normal distribution"}
# Make a tibble or dataframe with five curves

df <- tibble(x = seq(-3, 3, by = 0.001),
             t1 = dt(x, 3),
             t2 = dt(x, 5),
             t3 = dt(x, 7),
             norm = dnorm(x)) %>% 
  pivot_longer(cols = 2:5, names_to = "Distribution", values_to = "Density") # Transform into three columns

# Plot
ggplot(df, aes(x = x, y = Density)) +
  geom_line(aes(color = Distribution)) +
  scale_color_viridis_d(labels = c("Normal", "t, df = 3", "t, df = 5", "t, df = 7")) +
  xlim(c(-3, 3))
```

The concept of degrees of freedom alone can be covered more in-depth [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwj6guTEn9zxAhWSFlkFHbpRDrcQFjAJegQIAxAD&url=http%3A%2F%2Fweb.pdx.edu%2F~stipakb%2Fdownload%2FPA551%2FDegreesOfFreedom.pdf&usg=AOvVaw31ywhyELwYqUyDvu85-19L). 

## Exponential Distribution {#Exponential}

Recall the earlier section in which we discussed [Poisson](#Poisson) processes. The exponential distribution models a waiting time with $\lambda$ as the rate. This is similar to the [geometric](# geometric) distribution, which models the number of failures until one "success"; however, in this case, we are considering events and not successes/failures.

If $\lambda$ is the rate of a Poisson process, the density function can be written as:

$$f(y)=\lambda e^{-\lambda y} \quad \textrm{for} \enspace y>0$$

Thus, the probability that $Y=y$ can be interpreted as "the likelihood that two events occurred in y units of time."

Let's return to the adoption center example. Recall that the center has 10 adoptions per month on average. What is the probability that two pets are adopted in a week?

If each month has 30 days then people are adopting $1/3$ of a pet each day on average (even though you can't adopt a fraction of an animal!). We can then determine the rate as $\lambda=1/3$. 

Plugging the parameter $y<7$ and $\lambda=1/3$:

$$\begin{split}
P(Y < 7) &= \int_{0}^{7} \frac{1}{3} e^{-1/3y}dy \\ &= \frac{1}{3} \int_{0}^{7} e^{-1/3y}dy \\ &= \frac{1}{3} [-3e^{-1/3y}]^{7}_{0} \\ &= 1-\frac{1}{e^{7/3}} \approx 0.903
\end{split}$$

Our final answer can be thought of as: "There is a 90.3% chance probability that two animals are adopted before the end of a week."

If you prefer to avoid calculus, you can also use R to answer this problem using `pexp`, the corresponding distribution function.

```{r, pexp}
pexp(7, rate = 1/3, lower.tail = TRUE) # i.e. P(Y < 7) since lower.tail = TRUE
```

Why can't we we simply find the the sum of the densities `1:6`? Remember that the exponential distribution is continuous, not discrete! So only calculating the individual densities at integer values would give us an underestimate.

```{r, pexp_alt}
sum(dexp(1:6, 1/3))
```

Instead, we are required to provide an integral. To write a definite integral using R, we must provide a function and the two bounds.

```{r, expfunction}
exp_fun <- function(y) 1/3*exp(-1/3*y) # Write exponential function
integrate(exp_fun, 0, 7)
```

How can we visualize our interval between 0 and 7 days? Let's again use `stat_function` to draw a curve.

```{r, exp correct, eval=F}
exp_correct <- ggplot() +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3),
    xlim = c(0, 7),
    geom = "area",
    fill = "skyblue1"
  ) +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3), # Remember, the arguments must be provided in a list
    xlim = c(0, 30),
    geom = "function", # Choose a curve as the geometry
    size = 0.5) +
  geom_segment(aes(x = 0:30, y = 0, xend = 0:30, yend = dexp(0:30, rate = 1/3)), size = 0.5) +
  labs(x = "Days Between Adoptions", y = "Density")
```
```{r, exp comparison, fig.align="center",out.width="60%", echo=FALSE, fig.cap= "Correct and incorrect layering of exponential distribution layers"}
exp_correct <- ggplot() +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3),
    xlim = c(0, 7),
    geom = "area",
    fill = "skyblue1"
  ) +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3), 
    xlim = c(0, 30),
    geom = "function", 
    size = 0.5) +
  geom_segment(aes(x = 0:30, y = 0, xend = 0:30, yend = dexp(0:30, rate = 1/3)), size = 0.5) +
  labs(x = "Days Between Adoptions", y = "Density", title = "Correct Layering")

exp_incorrect <- ggplot() + 
  stat_function(
    fun = dexp,
    args = list(rate = 1/3), 
    xlim = c(0, 30),
    geom = "function", 
    size = 0.5) +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3),
    xlim = c(0, 7),
    geom = "area",
    fill = "skyblue1"
  ) +
  geom_segment(aes(x = 0:30, y = 0, xend = 0:30, yend = dexp(0:30, rate = 1/3)), size = 0.5) +
    labs(x = "Days Between Adoptions", y = "Density", title = "Incorrect Layering")

plot_grid(exp_correct, exp_incorrect)
```

Notice how the plot on the left looks much better? They are also placed so that the blue fill does not cover the curve? The jagged edges on the right plot results from placing the function geometry before the area geometry.

Because a large majority of the plot below 7 is colored in, it is quite likely that 2 pets would be adopted in a week. As the number of days "waiting" approaches 30, the probability approaches 0. 

***

By what point after one adoption can we begin expecting another? To answer this question, we can look for the median of the distribution. 

Recall that "q" functions return a x-value after providing a quantile in the distribution. Here, we'll use `qexp`.

```{r, qexp}
qexp(0.5, rate = 1/3)
```

The other measure of center for an exponential distribution is the mean. This is much easier to solve; if K is a Poisson random variable, $E(K)=\frac{1}{\lambda}$. So on average, we can expect an adoption on day $\frac{1}{1/3}=3$.

***

Imagine that you are working for a rescue shelter for pets and want to present the information that you just learned--that there are about 3 days on average between any two adoptions in a month.  

We'll build a deliverable visualization using the power of `ggplot`, `ggimage`, as well as your knowledge about the exponential distribution.

First, load the new `ggimage` package. This allows us to include image files and other graphic objects in our visualizations. To learn more about it, you can read ggimage's documentation [here](https://cran.r-project.org/web/packages/ggimage/index.html).

```{r, load ggimage}
library(ggimage) # Remember to use install.packages("ggimage") first
```

In particular, we'll need the package to include emojis of dogs and illustrate the average time period between adoptions. Think of adding the emojis as adding any other geometry. Beyond a coordinate position, the `geom_emoji` function requires an aesthetic called "image." The special unicode required for dogs is "1f415". We can also set the size of the image similar to geometries like `geom_point`.

```{r, fig.align="center",out.width="20%", echo=FALSE, fig.cap = "Dog emoji"}
ggplot() +
  geom_emoji(aes(x = 1, y = 1, image = "1f415"), size = 0.3) +
  theme_void()
```

...

```{r, dogplot, fig.align="center", out.width="60%", fig.cap= "Exponential plot with dogs"}
ggplot() +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3),
    xlim = c(0, 3),
    geom = "area",
    fill = "skyblue1"
  ) +
  stat_function(
    fun = dexp,
    args = list(rate = 1/3), 
    xlim = c(0, 5),
    geom = "function",
    size = 0.5) +
  geom_segment(aes(x = 0:5, y = 0, xend = 0:5, yend = dexp(0:5, rate = 1/3)), size = 0.5) +
  labs(x = "Days Between Adoptions", y = "Probability") +
  geom_emoji(aes(x = c(0.05,3), y = c(dexp(c(0,3), rate = 1/3)) + 0.025, image = "1f415"), size = 0.1) +
  annotate(geom = "text", x = 2.75, y = dexp(1, 1/3) + 0.01, 
           label = "On average, two pets are adopted every three days") +
  theme_classic()
```

***

## Gamma Distribution {#Gamma}

The exponential distribution is quite similar to the [geometric](#geometric) distribution since they both consider the amount of time it takes for one event to occur. The main difference between the two is that geometric random variables model Bernoulli or binary processes and exponential random variables model poisson ones. Bernoulli processes consider independent trials, so if each trial is a day then we would ignore the possibility of having more than one adoption or "success" per day. Some might consider changing the trial unit to each animal in the shelter, but we cannot determine the probability of each animal being adopted, and we also can't assume that they each animal's probability is the same. 

If $N$ is based on poisson processes and has parameter $\lambda$, the rate, $N$ represents the amount of time we must wait until $r$ events, thus following the **gamma distribution**.

$$ P(N=n)=\frac{\lambda^r}{\Gamma(r)}n^{r-1}e^{-\lambda n} \quad \textrm{for} \enspace k>0$$
If the $\Gamma$ (Gamma) function in the equation looks new to you, you can read more about it [here](https://www.cantorsparadise.com/the-beautiful-gamma-function-and-the-genius-who-discovered-it-8778437565dc).

When $r=1$ the equation simplifies to the exponential distribution, making it a special case similar to the geometric distribution.

$$ \begin{split}f(k)&=\frac{\lambda^1}{\Gamma(1)}k^{1-1}e^{-\lambda k} \\
&= \frac{\lambda}{1}k^0e^{-\lambda k} \\
&= \lambda e^{-\lambda k}
\end{split}$$

Returning to the adoption example, the gamma distribution lets us figure out the number of days that it takes for **more than** one adoption to occur in an interval of time.

For example, what is the probability that it takes less than 2 weeks for 3 pets to be adopted if 10 pets are adopted every month?

Recall that $\lambda = 1/3$, assuming that an adoption is equally likely to occur every day of the month. Therefore, if $K$ is a gamma random variable for the number of days it takes for 3 adoptions to occur, we can write the probability of $K<14 \enspace \textrm{days}$

$$ P(K<14) = \int_0^{14} \frac{{1/3}^3}{\Gamma(3)}k^{3-1}e^{-k/3}dk \approx 0.844$$

Or, two ways using R:

```{r, pgamma and integral}
# Using the gamma distribution function
pgamma(14, shape = 3, rate = 1/3)

# Is the same as writing this definite integral
gamma_fun <- function(k, lambda = 1/3, r = 3) (lambda^r)/(gamma(r)) * k^(r-1) * exp(-lambda * k)
integrate(gamma_fun, 0, 14)
```

So there is an approximate 84.4% probability that the center has 3 adoptions in two weeks. How do you think that this probability changes as we alter $r$ or $\lambda$?

```{r, gamma plot, fig.align="center", out.width="60%", warning=FALSE, message=FALSE}
# Create a vector of "day" values
x <- seq(0, 30, by = 0.01)

# Create a tibble with four gamma distributions, each with different r and lambda values
df <- tibble(x, 
             gamma_1 = dgamma(x, 3, 1/3), 
             gamma_2 = dgamma(x, 5, 1/3),
             gamma_3 = dgamma(x, 5, 2/3),
             gamma_4 = dgamma(x, 5, 1/6)) %>% 
  # Transform the data frame into longer format
  pivot_longer(cols = 2:5, names_to = "Distribution", values_to = "Density")

# Now plot!
ggplot(df, aes(x = x, y = Density, color = Distribution)) +
  geom_line(aes(group = Distribution)) +
  labs(x = "Days", y = "Density") +
  scale_color_viridis_d(labels = c("r = 3, lambda = 1/3",
                                "r = 5, lambda = 1/3",
                                "r = 5, lambda = 2/3",
                                "r = 5, lambda = 1/6"))
```

***

## χ2 Distribution

The $\chi^2$ distribution is extremely helpful in the statistics world, whether used for hypothesis testing as well as modeling. For instance, **$\chi^2$ tests of independence** are used to determine whether associations between variables/categories are *significant*. The test statistic that results from **$\chi^2$ goodness of fit tests** also follow a $\chi^2$ distribution and is equipped when researchers want to investigate whether observed data follow certain ratios. Although modeling is beyond the lens of this guide, the test statistic from [likelihood ratio tests](https://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm) follow a $\chi^2$ distribution and assess whether newer and larger models significantly improve upon older and simpler ones. 

We now know the uses of the $\chi^2$ distribution. But what actually is it? In a previous section, we learned that the [gamma](#gamma) distribution models the probability of $r$ events occurring in a certain period of time, given that it is based on Poisson processes. The $\chi^2$ distribution is a specific case of the gamma distribution in which the parameters $\lambda = 1/2$ and $r=k/2$ where k is the degrees of freedom. After plugging in these parameters, this form of the gamma distribution can be written as:

$$f(x)=\frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\Gamma(k/2)} \enspace \textrm{for} \enspace x \geq 0$$

Below is a plot of several $\chi^2$ distributions with different degrees of freedom.

```{r, multChisq, fig.align="center",out.width="60%", fig.cap= "Chi-square Distributions with 1, 4, and 8 degrees of freedom", warning=FALSE, message=FALSE}

x <- seq(0, 10, by = 0.2) # Vector of possible values

chisq_1 <- dchisq(x, 1)
chisq_2 <- dchisq(x, 4)
chisq_3 <- dchisq(x, 8)

df <- tibble(x, chisq_1, chisq_2, chisq_3) %>%
  rename(`1` = chisq_1, `4` = chisq_2, `8` = chisq_3) %>% 
  pivot_longer(cols = 2:4, names_to = "Degrees of Freedom", values_to = "Density")

ggplot(df, aes(x = x, y = Density, color = `Degrees of Freedom`)) +
  geom_line() +
  scale_x_continuous(name = "Value", breaks = seq(0, 10, by = 2))
```

***
### Chi-square and Normal Relationship

We previously learned that random deviates from the normal distribution can be taken using `rnorm`. All Chi-square distributions can be thought of as the sum of these squared and independent deviates. We can therefore also write the Chi-square distribution with k degrees of freedom as:

$$\sum_{i=1}^{k}Z_i^2 = Z_1^2 + Z^2_2 + \ldots + Z^2_k \sim \chi^2_{(k)}$$

Where each of the $Z^2_1,Z^2_2\ldots Z^2_k$ are independent and squared random variables following the standard normal distribution. 

The simplest chi-square distribution is derived from one squared and standard normal random variable (i.e. mean = 0, standard deviation = 1). If the new random variable $Y=Z_1^2$, then we can define Y as a chi-square random variable with $k=1$ degrees of freedom, $\chi^2_{(1)}$. 

The plot below shows the relationship between the two random variables. It is difficult to decipher visually, but the densities for each value of the chi-square distribution are the same as normal distribution at the square root of those values. This perhaps most obvious at the intercept at `x=1` since the square root of 1 is 1. We can also draw a segment between 3 and $\sqrt{3}$.


```{r, chisqnormal, fig.align="center", fig.cap = "Plot of the Chi-square distribution with 1 df and standard normal distribution",out.width="60%", warning=FALSE, message=FALSE}
ggplot() +
  stat_function(fun = dnorm,
                color = "darkblue") +
  stat_function(fun = dchisq,
                args = list(df = 1),
                color = "forestgreen") +
  annotate(label = "Normal Distribution", geom = "label", x = 0.5, y = 0.15, color = "darkblue") +
  annotate(label = "Chi-square Distribution", geom = "label", x = 2, y = 0.3, color = "forestgreen") +
  labs(x = "Value", y = "Density") +
  xlim(0, 4) +
  ylim(0, 0.5)
```

We can use this specific chi-square distribution with one degree of freedom to find the cumulative densities in a normal distribution. All that we have to do is acknowledge that our chi-square random variable is a standard normal random variable squared. In other words, if $P(Z=x)=P(Z^2=x^2)$, i.e. the cumulative densities do not change after rewriting this chi-square distribution as the standard normal distribution squared.

For instance, the probability that a normal random variable $Z$ is between -1.28 and 1.28 can be calculated with two methods: first by using the standard normal distribution and the second using the chi-square distribution with $k=1$ degrees of freedom.

$$ P(-1.28 < Z <1.28) \approx 0.8 \\ P(Z^2<1.28^2) = P(|Z| < 1.28) =(\chi^2_{(1)}<1.6384) \approx 0.8$$

Similarly with R,

```{r}
pnorm(1.28) - pnorm(1.28, lower.tail = FALSE)
pchisq(1.6384, df = 1)
```

Both of which can be rounded to 0.8.

We eventually use the chi-square distribution to observe more complicated situations with multiple independent and normal processes. For more the derivation of chi-square distributions with greater than 1 degrees of freedom, or if you want to learn more about this topic, see a full proof [here](https://en.wikipedia.org/wiki/Proofs_related_to_chi-squared_distribution).

***
### Using Chi-square Distributions for Testing

Similar to previous distributions used for testing, we must reasonably assume that each observation in the data is independent of each other. The size condition can be confirmed by looking at whether each category in the analysis has at least $x$ observations. In general, statisticians look to make sure that the observed counts are greater than 5, but the threshold can change depending on the circumstance.

The degrees of freedom can be calculated as $k=(n-1)(i-1)$ where the sample as $n$ observations and there are $i$ categories. Similar to the t-distribution, the degrees of freedom increases as we use larger data sets with more categories or observations.

After we have determined the degrees of freedom, we calculate the $\chi^2$ test statistic using the following:

$$ \begin{split} 
&\chi^2 =  \sum_{i=1}^{n}\frac{(O_i-E_i)^2}{E_i} \\
\textrm{Where} \enspace &O_i \enspace \textrm{is the observed count of the ith category} \\ 
&E_i \enspace \textrm{is the expected count of the ith category}
\end{split}$$

The function `chisq.test` also accomplishes this. What you provide in the function depends on whether you are doing a test for goodness of fit or independence.

For **goodness of fit** tests (whether observed data follows expected ratios), you need to create two vectors:

``` {r}
# Create vector of probabilities
predicted <- c(0.6, 0.3, 0.1)

# Create vector of observed values
observed <- c(66, 27, 15)

chisq.test(observed, p = predicted)
```

For tests of **independence** (whether associations exist between rows and columns), you need to provide a matrix:

```{r}
 matrix <- matrix(c(103, 150, 103, 102, 140, 97),
                 nrow = 2, ncol = 3, byrow = TRUE, 
                 dimnames = list(c("Group A", "Group B"), 
                                 c("Variable 1", "Variable 2", "Variable 3")))

knitr::kable(matrix)

chisq.test(matrix)
```

Great! This concludes the last distribution in this guide. Thank you so much for reading this guide, and I wish you luck as you continue your statistics journey! :)


<!--chapter:end:05-other_continuous.Rmd-->

