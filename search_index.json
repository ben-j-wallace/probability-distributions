[["index.html", "Other Continuous Distributions t-Distribution Chapter 1 Introduction", " Other Continuous Distributions t-Distribution Ben Wallace 2021-07-12 Chapter 1 Introduction This is a guide to understanding and visualizing several important discrete and continuous distributions in the statistics world. We will use several R packages in the process. stats (installed by default in RStudio) to retrieve statistical distributions. tibble to coerce data frames into tibbles. What is a tibble? ggplot2 to produce geometries from visualizations. install.packages(&quot;tidyverse&quot;) # Includes both tibble and ggplot2 packages # or the development version # devtools::install_github(&quot;tidyverse&quot;) This guide is intended for those that are new to statistics and/or data science and are somewhat familiar with the R language. If you would like a more comprehensive introduction to R, I recommend both of these free and accessible books: R for Data Science R for Statistics I find it helpful to begin this guide with an overview of the function ggplot, which will be the first chapter. This includes an important conceptual basis for aesthetics, geometries, mappings, and scales. The lessons learned in ggplot chapter will then be applied to the remainder of the book. In each chapter, we will introduce a statistical distribution and use ggplot to better visualize our concepts. We will also go over several important functions from the stats package including d functions, which return a vector of densities. p functions, which give us a cumulative probability of the distribution (also known as a distribution function). q functions returns a probability corresponding to a given quantile. and finally, r functions generate random values from a given distribution. "],["ggplot.html", "Chapter 2 Basic Concepts of ggplot 2.1 Geometries and Aesthetic Mappings 2.2 Altering Aesthetics and Scalings", " Chapter 2 Basic Concepts of ggplot A visualization can be anything. It can be a drawing, a photograph, a sculpture, a well-decorated cake you get the idea. However, data scientists and statisticians are often not very skilled at all of these art forms, so they are limited to their tools at hand: functions, code, and geometries. This leads us to data visualizations. What separates a data visualization from a drawing is the tools used to construct it. In this guide, our blank canvas is a function called ggplot. library(tidyverse) ggplot() Figure 2.1: An empty plot 2.1 Geometries and Aesthetic Mappings Everytime that we want to visualize something, we must use this function. The stuff that fills up this space are called geometries. Here are just a few that ggplot has avaiable to us. geom_rect produces rectangles. geom_point creates dots. geom_line draws lines. geom_bar makes a barplot. geom_function uses functions to draw a continuous curve. If the geometries are the shapes on our ggplot canvas, how do we put them on there? This is where aesthetics and mapping come in. When we call the function ggplot we need to connect our data to aesthetic mappings which are then applied to various geometries. But where do we use aesthetics mappings? The mapping argument in the ggplot and geom functions are what connect variables from our dataset. Lets look at a dataset of Marvel superheroes and apply the following aesthetics: Years since joining Marvel  x Appearances  y Gender  color Now we will use the aes function to define our mappings for a simple scatterplot. We will also use the labs function to label our axes and title and an additional size argument to make our points larger. ggplot(data = avengers, mapping = aes(x = years_since_joining, y = appearances, color = gender)) + geom_point(size = 5) + labs(title = &quot;&#39;Age&#39; of Superheroes and Appearances&quot;, x = &quot;Years since Joining Marvel&quot;, y = &quot;Appearances&quot;) Figure 2.2: Basic Avengers scatterplot We used two arguments in the gpplot function: data and mapping. Next, we added the geometry geom_point using a plus sign (+). The aesthetic mapping in the ggplot function are then passed onto geom_point. Even though we must include data and mappings to produce a visualization, we do not always have to name the arguments themselves. For instance, we could simply write: ggplot(avengers, aes(x = years_since_joining, y = appearances, color = gender)) + geom_point() Aesthetic mappings can also produce different geometries by simply changing the function. For example, instead of using geom_point we can use geom_rect to draw rectangles or squares. ggplot(avengers, aes(x = years_since_joining, y = appearances, fill = gender)) + geom_rect(aes(xmin = years_since_joining - 1, xmax = years_since_joining + 1, ymin = appearances - 100, ymax = appearances + 100)) + labs(title = &quot;&#39;Age&#39; of Superheroes and Appearances&quot;, x = &quot;Years since Joining Marvel&quot;, y = &quot;Appearances&quot;) (#fig:avengersplot_rect)Avengers scatterplot with rectangles Notice in Figure ___ that the geom_rect geometry requires aesthetic mappings beyond those in geom_point, including the minimums and maximums for x and y. The color aesthetic of geom_point turns to fill in geom_rect since the rectangles are not points; they are shapes with empty space. So far we have established that aesthetics can be passed onto different geometries. Figure __ recaps where we are at now. (#fig:ggplot_diagram)ggplot diagram Generally, we want to provide mappings to the ggplot function so that they pass to all of our geometries. Including them in every line of code would be repetitive. # Don&#39;t do this! ggplot(avengers, aes(x = years_since_joining, y = appearances, color = gender)) + geom_point(aes(x = years_since_joining, y = appearances, color = gender)) # Do this instead! ggplot(avengers, aes(x = years_since_joining, y = appearances, color = gender)) + geom_point() But there are plenty of instances in which we wouldnt want to keep our aesthetics limited to the ggplot function linefor example, plots with two different groups or those with different scales on the y axis (although it is not generally recommended to have multiple scales on an axis since it is confusing). Lets return to the Avengers plot but keep the color argument in geom_point so that the color of our text doesnt differ by gender. ggplot(avengers, aes(x = years_since_joining, y = appearances)) + geom_point(aes(color = gender), size = 5) + geom_text(aes(label = name_alias), size = 3) + labs(title = &quot;&#39;Age&#39; of Superheroes and Appearances&quot;, x = &quot;Years since Joining Marvel&quot;, y = &quot;Appearances&quot;) (#fig:avengersplot_gender)Avengers scatterplot with unique color and label aesthetic assignments Figure ___ doesnt look pretty but you get the idea. A general rule to follow with data visualizations is to avoid mapping one aesthetic to multiple variables. For instance, a plot in which the color is used to illustrate both an Avengers gender and their name would be confusing. # Don&#39;t do this! ggplot(avengers, aes(x = years_since_joining, y = appearances)) + geom_point(aes(color = gender), size = 5) + geom_text(aes(color = name_alias, label = name_alias), size = 3) + labs(title = &quot;&#39;Age&#39; of Superheroes and Appearances&quot;, x = &quot;Years since Joining Marvel&quot;, y = &quot;Appearances&quot;) Sadly, Steve Rogers is not a gender and neither are the other Avenger names in the legend. This is why we should not map one aesthetic to more than one variable. 2.2 Altering Aesthetics and Scalings Now, lets introduce a way to change our aesthetics further. There are some arguments within geom functions that allow us to do this. For instance, you might have noticed that I used size to make points and labels larger in the previous plots. We can also use color or fill to make adjustments to our geoms. ggplot(avengers, aes(x = years_since_joining, y = appearances)) + geom_point(color = &quot;tomato&quot;, size = 5) (#fig:avengersplot_red)Avengers scatterplot in red! But previously we have related color with an Avengers gender. ggplot uses some default colors to differentiate male and female Avengers but we can change this further with scales. Every scale function has three components: scale_ The name of the aesthetic we are scaling. In this case, color. A method of applying the scale. In this case, well use manual. So to combine all of the previous steps together, we would call the functionscale_color_manual to directly alter the colors of our aesthetic related to gender. ggplot(avengers, aes(x = years_since_joining, y = appearances, color = gender)) + geom_point(size = 5) + scale_color_manual(values = c(&quot;Tomato&quot;, &quot;Navy&quot;)) (#fig:avengersplot_scale)Avengers scatterplot using scale_color_manual Weve not only learned about the gender gap in the Marvel universe but also how to apply scales to our aesthetics! There are a number of scales that we can use further. For example, scale_alpha_binned allows us to alter the transparency of points after providing a range between 0-1. The breaks argument separates our observations into different groups. ggplot(avengers, aes(x = years_since_joining, y = appearances, alpha = appearances)) + geom_point(size = 5) + scale_alpha_continuous(range = c(0.2, 1), breaks = c(2000, 3000, 4000)) (#fig:avengersplot_alpha)Avengers scatterplot with scale_alpha_continuous An important fact to remember is that mapping a variable to more than one aesthetic sometimes exaggerate differences in data. In this case, using two aestheticsy and alphafor appearance count draws the eye to the Avengers with the most appearances. This is why any data scientist must consider the principles of data visualization and how certain techniques mislead or distort data. So far, we have learned how to call the ggplot function, apply aesthetic mappings, layer geometries, and alter aesthetics with scales. Any data visualization, regardless of its complexity, follows this formula. Hopefully this chapter has provided you a some more confidence as you begin producing your own visualizations. However, this is by no means an exhaustive tutorial. To learn more about data visualizations here are some useful resources: R Cookbook Cookbook for R The R Graph Gallery R for Data Science Online Community The rest of this guide specifically uses our new visualization tool to learn about the statistical distributions that govern our world. Click on to learn more! "],["Distributions.html", "Chapter 3 Understanding Distributions 3.1 Uniform Distribution", " Chapter 3 Understanding Distributions Every distribution discovered in the probability world is determined by a random variable. These variables are the not the same ones that you were exposed to in algebra or calculus class. Instead, they are used to map random processes, like rolling dice or playing the lottery, to readable notation. Well begin with a simple example of this by flipping a coin. Let the random variable X (usually random variables are capital letters) denote this process. It can take two forms, heads or tails. We can use a binary 0 or 1 to show this: \\[X = \\{0,1\\}\\] You might be asking yourself: What does this have to do with probability? We begin by defining the random variable so that we can later discover its mathematical attributes, such as its mean or variance, for example. We can also define the probability that the random variable \\(X\\) (the coin) takes the form of heads or tails by using probability notation. \\[P(X = x)\\] Where x can equal 0 or 1. we can interpret the statement above as the probability that the random variable X equals x. So what is that probability? By common sense, we can conclude that the probability that you get heads is \\(P(X=1)=1/2\\). We can extend this logic to show that the sum of probabilities for any random variable is 1. \\[P(X = 0) + P(X = 1)= 1/2 + 1/2 = 1\\] In other, more complicated probability distributions, we use random variables such as \\(X\\) to mark these processes without having to explain them each time. 3.1 Uniform Distribution Well begin our journey with uniform distribution. Given \\(a &lt; b\\), we can define this distributions density as: \\[ P(X=x) = \\frac{1}{b-a}\\quad \\textrm{for} \\enspace a \\leq x \\leq b \\] When we refer to densities we are talking about probabilities. The two terms are interchangeable, but the term density emphasizes the finite probability space that a distribution occupies. As mentioned previously, the sum of probabilities for any distribution is 1. How do we get this equation? Imagine drawing a rectangle in the interval (a,b) with height 1/(b-a). ggplot() + geom_rect(aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = NA, color = &quot;black&quot;) + labs(x = &quot;x&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = c(0,1), labels = c(&quot;a&quot;, &quot;b&quot;)) + scale_y_continuous(breaks = c(0,1), labels = c(&quot;&quot;, &quot;h&quot;)) Figure 3.1: Remember Scales? See if you can understand the code We have the height as \\(\\frac{1}{(b-a)}\\). We know this since the cumulative area of a distribution must be 1 and the width is b-a. Using the area of a rectangle with h being the height of the uniform distribution. \\[(b-a)h=1\\] For the equation to hold, h must equal 1/(b-a). Lets define X as a discrete random variable that is uniform on 1 through 10. The term discrete means that X can only be integers, and the fact that its probabilities are uniform means that every integer has an equal probability of occurring. Therefore: \\[ \\begin{split} &amp;X=\\{1,2,...,10\\}\\\\&amp;P(X=x)=\\frac{1}{10-1}=1/9\\quad\\textrm{for}\\quad1\\leq x\\leq 10 \\end{split} \\] The first line outlines the random variable X and the numbers that it can take. The second line then calculates the probability of the random variable taking the integers 1-10 as 1/9. In other words, the density, or individual probability, for each of \\(1\\leq x\\leq 10\\) is 1/9. The first stats function we can use with the uniform distribution is dunif which also returns the density at point x in the distribution given left and right bounds. This is quite easy to determine in the uniform distribution because every point has the same density of h- in other words, the height. dunif has 3 arguments. A vector of values to calculate densities for. The minimum or min of the distribution. The maximum or max of the distribution. Well return to our example where the beginning and end of the uniform distribution is at x = 1 and x = 10, respectively. What are the densities at x = 1, 5, and 10? dunif(c(1, 5, 10), min = 1, max = 10) ## [1] 0.1111111 0.1111111 0.1111111 You are correct if you guessed 1/9 or approximatey 0.111! If \\(a = 1\\), \\(b = 10\\) then by definition \\(h = 1/9\\). Thus, as we calculated earlier, every point in the distribution has a density of 1/9. We can visualize this by drawing a rectangle using geom_rect and then adding points with geom_point. Remember that certain geometries require aesthetics beyond x and y. ggplot() + geom_rect(aes(xmin = 1, xmax = 10, ymin = 0, ymax = 1), fill = NA, color = &quot;black&quot;) + labs(x = &quot;x&quot;, y = &quot;Density&quot;) + geom_point(aes(x = c(1, 5, 10), y = 1), size = 5, color = &quot;navy&quot;) The punif() function helps us find the cumulative density between at the qth quantile. Using a different rectangle with \\(a = 0\\), \\(b = 4\\), and \\(h = 1/4\\), we can find the proportion of the distribution between the minimum of 0 and maximum of 2 by writing the following: punif(2, 0, 4, lower.tail = TRUE) # Remember: we don&#39;t always have to name our arguments if we know the order. ## [1] 0.5 We can similarly write: sum(dunif(0:1, min = 0, max = 4)) # Not including 2. ## [1] 0.5 There are three arguments in this function: a vector of quantiles, a minimum, maximum, and a binary lower.tail argument. By default this is set to TRUE. We can also think about the punif() function as drawing a smaller rectangle from 0 to 2 (if lower.tail = FALSE) and calculating its area. Below, this is the same as the percent of the total area that the navy rectangle occupies. ggplot() + geom_rect(aes(xmin = 0, xmax = 2, ymin = 0, ymax = 1/4), # Small rectangle alpha = .2, fill = &quot;navy&quot;, color = &quot;navy&quot;, linetype = &quot;dashed&quot;) + geom_rect(aes(xmin = 0, xmax = 4, ymin = 0, ymax = 1/4), # Large rectangle fill = NA, color = &quot;black&quot;) + labs(x = &quot;x&quot;, y = &quot;Density&quot;) \\[ area \\hspace{0.2 cm}= \\hspace{0.2 cm}base \\hspace{0.2cm} * \\hspace{0.2 cm}height\\] \\[area = 2*0.25=0.5\\] Now we will consider the opposite scenario where we want to find the x-value that correponds with the 50th percentile of our distribution. This is the purpose of qunif(). We already know from using the distribution function punif() that this is 2. qunif(0.5, 0, 4) #lower.tail is also set to TRUE by default ## [1] 2 The last function runif() generates random deviates within the distribution. There are three arguments in this function: n, the number of deviates we want to produce min, the left bound of the uniform distribution max, the right bound of the uniform distribution Well use the round() function to round them to the hundredths place. # Create object of deviates unif_dev &lt;- round(runif(10, min = 0, max = 4), digits = 2) # Plot ggplot() + geom_rect(aes(xmin = 0, xmax = 4, ymin = 0, ymax = 1/4), fill = NA, color = &quot;black&quot;) + geom_point(aes(x = unif_dev, y = 1/4), size = 5, color = &quot;navy&quot;) + # Plot deviates labs(x = &quot;x&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(0, 4, by = 1), labels = seq(0, 4, by = 1)) "],["Discrete.html", "Chapter 4 Other Discrete Distributions 4.1 Geometric Distribution 4.2 Binomial Distribution 4.3 Negative Binomial Distribution 4.4 Poisson Distribution", " Chapter 4 Other Discrete Distributions 4.1 Geometric Distribution For the next distribution, imagine a basketball player that is not particularly good. Whenever he takes a free throw, there is a 10% probability that he makes it. How many free throws can we expect him to shoot before he makes one? The geometric distribution helps us answer this question. We can think of this as a spread of N trials required to reach a probability p in the following equation. \\[P(N = n) = (1-p)^{n} p \\hspace{0.7cm}for\\hspace{0.2cm} n=0,1,...,\\infty\\] Since trials can only be measured in integers, the geometric distribution is a type of discrete distribution. Although there is a more in-depth proof, the formula for this distribution is already quite intuitive. First, the geometric distribution needs n failures and one success for N to equal n. Likewise, if the probability of a success is represented by \\(p\\), the probability of a failure must be \\(1-p\\). When can we use a geometric distribution, and does it apply to the free throw example? In order to model a random process with the geometric distribution, it must meet the following assumptions: Every trial must be independent of each other. In other words, the outcome of one trial does not have any bearing on the other outcomes. There are only successes and failures. If a response has more than two outcomes, we should not use the geometric distribution. The probability of success remains consistent for each trial. It is possible that these assumptions may not be fully met in our case study. For example, does the basketball player get tired over time, thus decreasing the probability of a successful free throw? This would violate the third assumption. The geometric distribution has a mean or expected value of \\(\\frac{1-p}{p}\\). This means that if we had many similar basketball players shoot free throws, their long run average failures would be (\\(\\frac{1-0.1}{0.1}=9\\)), i.e. their average successful free throw would be on shot 10. Now that we have found the long run mean of the geometric distribution, lets see how we can visualize the first thirty shots that the basketball player takes. Again, well use a d(density) function (in this case called dgeom) to produce the individual densities for each of the shots in the total distribution. dgeom has 3 arguments. A vector of integers in order to return their corresponding densities. We can imagine this as the probability of \\(x\\) failures before the first success. Here, well use 0:30 to represent the integers 0, 1, 2,  , 29, 30. The prob or probability of a successful free throw. An optional log argument. # Create a data frame with the first 30 values, incremented by 1, as well as a geometric distribution. dgeom &lt;- tibble(failures = 0:30, density = dgeom(x = 0:30, prob = 0.1, log = FALSE)) # Create plot ggplot(dgeom) + geom_bar(aes(x = failures, y = density), stat = &quot;identity&quot;) + labs(x = &quot;Missed Shots&quot;, y = &quot;Density&quot;) Each ith value on the x axis and its corresponding jth y value can be conceptualized as There is a j probability that the basketball player had i failures before his first successful free throw. As suggested in the plot above, the probability that a shot is the first success approaches 0 as the observation number approaches infinity. Now lets talk about the stat argument in geom_bar. By default, this is set to \"count\". This means that the height of an x-values bar is determined by the number of times that it appears in a dataset. By changing this argument to \"identity\", ggplot looks for a corresponding y aesthetic to determine the height of the bar. If we dont alter the behavior of geom_bar we would get a plot that looks like this: # Don&#39;t do this! ggplot(dgeom) + geom_bar(aes(x = failures)) + labs(x = &quot;Missed Shots&quot;, y = &quot;Density&quot;) To calculate the probability of a range of outcomes we can use the function pgeom(). For example, how likely is it that the player can make a free throw in 9 or fewer shots? To find this, well plug in \\(q = 8\\). # Create a cumulative density function pgeom(8, 0.1) ## [1] 0.6125795 We plug in \\(q=8\\) instead of \\(q=9\\) because the first argument of function requires the number of failures until the first succcess. In other words, the value that pgeom returns can be thought of as the probability that we have less than or equal to 8 failures before the first successful free throw. When using a p or distribution function with lower.tail = TRUE, the first argument is our right bound. If we change to lower.tail = FALSE, the first argument becomes our left bound. We can think of our calculation above as the same as finding the area of the binomial distribution from 0 to 8i.e. the total area of the first nine rectangles. We will fill the first nine bars blue and the rest gray. ggplot(dgeom) + # Insert the vector of densities into the data argument geom_bar(aes(x = failures, y = density), stat = &quot;identity&quot;, fill = ifelse(dgeom$failures &lt;= 8, &quot;skyblue1&quot;, &quot;grey35&quot;)) + labs(x = &quot;Failed Shots&quot;, y = &quot;Density&quot;) Here are a few things to remember from the code above before we move on. -The ifelse(test, yes, no) function is in base R and allows us to set a condition for the fill aesthetic of our bar plot. We are essentially telling ggplot to fill each bar blue if the shot_num is less than or equal to 9. Otherwise, fill the rest of the bars gray. -By default, the lower.tail argument is set to TRUE, meaning that we are finding the area of the distribution from the first shot to the ninth shot. However, if we set the lower.tail argument to FALSE, we will get the area of the distribution after the eighth shot. pgeom(8, prob = 0.1, lower.tail = FALSE) ## [1] 0.3874205 # As represented on a bar plot ggplot(dgeom) + geom_bar(aes(x = failures, y = density), stat = &quot;identity&quot;, fill = ifelse(dgeom$failures &gt; 8, &quot;skyblue1&quot;, &quot;grey35&quot;)) + labs(x = &quot;Failed Shots&quot;, y = &quot;Density&quot;) The qgeom() function can be thought of as the inverse of pgeom(). Because of this, we enter in a cumulative density (value from 0-1) into the function and it returns the shot number in which we would expect to reach that cumulative density, from left to right on a number line. Like pgeom(), keep in mind that lower.tail is set to TRUE by default. By what shot can we begin expecting the player have already made a free throw? To answer this question, we will enter 50% as a cumulative density. qgeom(0.5, prob = 0.1) ## [1] 6 dgeom &lt;- dgeom %&gt;% mutate(cume_dist = cumsum(density)) ggplot(dgeom) + geom_bar(aes(x = failures, y = density), stat = &quot;identity&quot;, fill = ifelse(dgeom$cume_dist &lt;=.5, &quot;skyblue1&quot;, &quot;grey35&quot;)) + labs(x = &quot;Shot Number&quot;, y = &quot;Density&quot;) There is quite a bit to unpack here. We use mutate() to create a new variable for the cumulative density at each shot. This is the sum of a shots density and all of the previous shots before it. Next, we return to the ifelse() function to determine which bars to fill. We can interpret our code as If the cumulative density of this shot is less than or equal to .5, fill it blue; otherwise, fill it gray. Finally, imagine that 1,000 exact copies of the basketball player shoot free throws. Below is the distribution of those outcomes. The function rgeom() simulates each of the 1,000 players and returns the number of their first successful shot. rand_geom &lt;- tibble(deviates = rgeom(1000, 0.1)) ggplot(rand_geom) + geom_histogram(aes(x = deviates), binwidth = 2) + labs(x = &quot;First Shot&quot;, y = &quot;Frequency&quot;) + scale_x_continuous(breaks = seq(0, 80, 10)) # Added more ticks on x axis As stated at the beginning of this chapter, the geometric distribution can be thought of the spread of N trials required to get probability p. Because of this, as we increase the number of simulated basketball players, we reach a distribution more and more similar to the actual binomial distribution. To visualize this, well generate four plots: three randomly-sampled geometric distributions and a true geometric distribution. To simplify our code, well start by writing a new function called geom_plot. If function-writing is unfamiliar to you, I encourage you to read this friendly introduction. # Create a function that randomly samples from geometric distribution with sample size n geom_plot &lt;- function(prob, n){ temp &lt;- tibble(deviates = rgeom(n, prob)) plot &lt;- ggplot(temp, aes(x = deviates)) + geom_histogram(binwidth = 2) + scale_x_continuous(breaks = seq(0, 80, 20)) + labs(x = NULL, y = NULL, caption = str_c(n, &quot;simulations&quot;, sep = &quot; &quot;)) return(plot) } # Generate random samples and plots sample_1000 &lt;- geom_plot(0.1, 1000) + labs(y = &quot;Count&quot;, title = &quot;Random Deviates&quot;) sample_10000 &lt;- geom_plot(0.1, 10000) sample_100000 &lt;- geom_plot(0.1, 100000) # Generate true geometric distribution dgeom &lt;- tibble(shot_num = 1:80, density = dgeom(1:80, prob = 0.1, log = FALSE)) true_plot &lt;- ggplot(dgeom) + geom_bar(aes(x = shot_num, y = density), stat = &quot;identity&quot;, width = 0.5) + labs(x = &quot;First Shot&quot;, y = &quot;Density&quot;, title = &quot;Geometric Distribution&quot;) + scale_x_continuous(breaks = seq(0, 80, 20)) # Plot all together in a grid cowplot::plot_grid(NULL, true_plot, NULL, sample_1000, sample_10000, sample_100000, ncol = 3, nrow = 2) 4.2 Binomial Distribution While we would use the geometric distribution to model the number of failed free throws until a successful shot, the binomial distribution instead considers the probability of successful free throws in a fixed number of attempts. It is defined belowbelow: \\[P(K = k) = C_{n,k}p^{k}(1-p)^{n-k}\\\\\\textrm{where} \\hspace{0.2cm}C_{n,k}\\hspace{0.2cm}\\textrm{is the number of ways that we can pick k successes in n trials.}\\] At first glance, this formula is complicated, but it can be easily divided into two parts: \\(C_{k,n}\\) represents the number of ways we can select k successes in n trials. This combination can be calculated as \\(C_{n,k}=\\frac{n!}{k!(n-k)!}\\). \\(p^{k}(1-p)^{n-k}\\) or the probability of getting k successes and therefore n-k failures. We multiply the scenarios term by the probability term to account for the fact that there are multiple ways that we can get to the kth success. A basketball player does not need to make k shots in a particular order. For example, if we are interested in the probability that the basketball player shoots 30 free throws and makes 3 of them, we can plug these numbers into our equation. We have n = 30 independent trials, assuming that the player shoots at the same place and does not get tired. We want to have k = 3 successful free throws and n- k = 27 unsuccessful ones. Recall that the probability of a successful shot is p = 10%. Thus, a missed shot has probability 1 - p = 90%. Thus, our density formula allows us to find the probability of a basketball player making 3 shots using simple multiplication. \\[P(K = 3) = C_{30,3}(.10)^{3}(1-.10)^{27}\\approx.2361\\] With R, the function dbinom() similarly accomplishes this. # The arguments do not all need to be named, but I will do so here. dbinom(x = 3, size = 30, prob = .1) ## [1] 0.2360879 In other words, the probability that the basketball player shoots 30 free throws and makes exactly 3 of them is about 0.2361. As we begin visualizing this with ggplot(), we should return to our strategy from the geometric chapter by creating a data frame or tibble with two columnsone to signify all of the possible number of successes that we can have, and another of their corresponding probabilities or densities. dbinom &lt;- tibble(shots = 0:30, density = dbinom(0:30, size = 30, prob = .1)) We can now plot this with geom_bar(). ggplot(dbinom) + geom_bar(aes(x = shots, y = density), stat = &quot;identity&quot;) + labs(x = &quot;Successful Shots&quot;, y = &quot;Density&quot;) + scale_x_continuous(breaks = seq(0, 30, 2)) As seen above, it is quite unlikely that the basketball player will make more than 10 shots. In fact, their probabilities arent even large enough for the bars to register on the plot. In some scenarios in the statistics world, we arent only interested in the probability of getting exactly k successes; we may want to consider a range of outcomes. For example, a personal coach who records the basketball players progress wants to determine the probability that he makes 3 or fewer shots out of 30. Using the previous chapters, we can assume that we would want to use the distribution function pbinom() to answer this question. # Before you continue, think about what the four arguments mean. Why would we set the lower.tail to TRUE instead of FALSE? pbinom(3, 30, 0.1, lower.tail = TRUE) ## [1] 0.6474392 The first argument in the above function is similar to the other p functions since it can be thought of as the right bound of interest. We can imagine the pbinom() function as finding the area of the binomial distribution from 0 to 3. ggplot(dbinom) + geom_bar(aes(x = shots, y = density), stat = &quot;identity&quot;, fill = ifelse(dbinom$shots &lt;= 3, &quot;purple2&quot;, &quot;lightgray&quot;)) + labs(x = &quot;Successful Shots&quot;, y = &quot;Density&quot;) Just as we used it in the geometric distribution section, the ifelse() function allows us to fill our bars with different colors depending on the shot value. The interpretation here is fill the bar purple if number of successful shots is three or fewer. If not, fill the bar gray. Like the other r functions,rbinom() randomly simulates the outcomes of a distribution. In this case, the function returns the number of successful free throws made all of the simulations. This is useful because it helps us better understand the natural random process. ranbinom &lt;- tibble(deviates = rbinom(1000, 30, .10)) glimpse(ranbinom) ## Rows: 1,000 ## Columns: 1 ## $ deviates &lt;int&gt; 2, 1, 2, 6, 0, 2, 3, 6, 4, 1, 3, 4, 8, 1, 3, 0, 5, 3, 3, 3, 4~ ggplot(ranbinom) + geom_bar(aes(x = deviates), stat = &quot;count&quot;) + scale_x_continuous(breaks = seq(0,10, 1)) + labs(x = &quot;Successful Shots&quot;, y = &quot;Frequency&quot;) 4.3 Negative Binomial Distribution Next, well introduce the negative binomial distribution. Similar to the from the geometric distribution, we are interested in modeling the number of failures in a binary process. However, we are not only interested in how long it takes for one success; we are interested in modeling the number of failures until a particular number (\\(r\\)) of successes. \\[\\begin{equation} P(Y=y) = \\binom{y + r - 1}{r-1} (1-p)^{y}(p)^r \\quad \\textrm{for}\\quad y = 0, 1, \\ldots, \\infty. \\end{equation}\\] An interesting case appears when we set \\(r=1\\), i.e. the number of failures until one success. \\[\\begin{split} P(Y=1) &amp;= \\binom{y + 1 - 1}{1-1} (1-p)^{y} (p)^1 \\\\ &amp;= \\binom{y}{0} (1-p)^yp \\\\ &amp;=(1-p)^yp \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty \\end{split}\\] Notice how the last line of this looks exactly like the geometric distribution? This is because the geometric distribution is a special case of the negative binomial distribution. Now lets return to the basketball example. As we have established before, there is about a 4% chance that our player makes 8 failures before his first successful shot. What is the probability that he makes 8 failures before his second successful shot? dnbinom(8, 2, 0.1) ## [1] 0.03874205 dnbinom &lt;- tibble(failures = 0:30, density = dnbinom(0:30, 2, 0.1)) ggplot(dnbinom, aes(x = failures, y = density)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;turquoise1&quot;, color = &quot;black&quot;) + labs(x = &quot;Failed Shots&quot;, y = &quot;Density&quot;, title = &quot;Number of Failures until Second Free Throw&quot;) Now that we have a new parameter r, how does the distribution change as it increases or decreases? nbinom_plot &lt;- function(x, size, prob){ temp &lt;- tibble(failures = x, density = dnbinom(x, size, prob), cume_density = cumsum(density)) median &lt;- temp %&gt;% filter(between(cume_density, .5, .55)) plot &lt;- ggplot(temp, aes(x = failures, y = density)) + geom_density(stat = &quot;identity&quot;) + geom_vline(xintercept = median$failures[1]) + labs(x = NULL, y = NULL) return(plot) } p1 &lt;- nbinom_plot(0:100, 3, 0.1) + ggtitle(&quot;r = 3, prob = 0.1&quot;) p2 &lt;- nbinom_plot(0:100, 5, 0.1) + ggtitle(&quot;r = 5, prob = 0.1&quot;) + labs(x = &quot;Failures&quot;, y = &quot;Density&quot;) p3 &lt;- nbinom_plot(0:100, 7, 0.1) + ggtitle(&quot;r = 7, prob = 0.1&quot;) p4 &lt;- nbinom_plot(0:100, 5, 0.15) + ggtitle(&quot;r = 5, prob = 0.15&quot;) p5 &lt;- nbinom_plot(0:100, 5, 0.08) + ggtitle(&quot;r = 5, prob = 0.05&quot;) plot_grid(p1, NULL, p4, NULL, p2, NULL, p3, NULL, p5, ncol = 3, nrow = 3) As you can see, the center of the distribution moves right as \\(r\\) increases or the probability of a success decreases. 4.4 Poisson Distribution Unlike those based on Bernoulli processes (binary and independent outcomes), a Poisson process considers the number of times an event occurs in a given time or space. If \\(K\\), for instance, is the number of events in an interval, well model it with the Poisson Distribution. \\[ P(K=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!} \\quad \\textrm{for} \\quad \\mathrm{k} = 0, 1, \\ldots, \\infty \\\\ \\textrm{Where} \\enspace \\lambda \\enspace \\textrm{is the average number of events in a unit of time} \\] Now, well put this formula into practice. Consider a pet shelter which has an average of 10 adoptions per month. Every month, they rescue 9 pets. If the shelter currently has 40 pets, what is the probability that have less than 45 pets at the end of the month in order to prevent overcrowding? From above, we can deduce that the shelter would need to have more than 4 adoptions in order to prevent overcrowding. \\[ \\begin{split} \\textrm{Current pets} \\enspace + \\enspace \\textrm{Rescues} \\enspace - \\enspace \\textrm{Adoptions} &amp;= \\textrm{Total pets} \\\\ 40 + 9 - \\textrm{Adoptions} &amp;&lt;45 \\\\ \\textrm{Adoptions} &amp;&gt; 4 \\end{split} \\] Plugging in \\(\\lambda = 10\\) and \\(k=5,6,7,\\ldots40\\) \\[ \\begin{split} P(K=5) + P(K=6) \\ldots + P(K=40) &amp;= \\frac{e^{-10}10^5}{5!} &amp;+ \\frac{e^{-10}10^6}{6!} &amp;+ \\ldots + \\frac{e^{-10}10^{40}}{40!} \\\\ &amp;=\\frac{2500}{3e^{10}} &amp;+ \\frac{12500}{9e^{10}} &amp;+ \\ldots + \\frac{10^{40}}{e^{10}40!} \\\\ &amp;= 0.03783 &amp;+ 0.06305 &amp;+ \\ldots +0 \\\\ &amp;\\approx 0.97075 \\end{split}\\] There is about a 97% probability that the pet rescue center has at least 5 adoptions. In other words, its quite likely that they will not have overcrowding. Using R, we can use the density function dpois and more easily calculate the densities from 5 to 40 adoptions. sum(dpois(5:40, lambda = 10)) ## [1] 0.9707473 The ppois distribution function also work here. ppois(4, 10, lower.tail = FALSE) # 4 is not included ## [1] 0.9707473 adoptions &lt;- tibble(num_adopt = 0:40, density = dpois(0:40, 10)) ggplot(adoptions, aes(x = num_adopt, y = density)) + geom_bar(width = 0.5, stat = &quot;identity&quot;, fill = ifelse(between(adoptions$num_adopt, 5, 40), &quot;orchid1&quot;, &quot;black&quot;)) + labs(x = &quot;Number of Adoptions per Month&quot;, y = &quot;Density&quot;) As we have established, the probability mass function for a poisson distribution has a mean of \\(E(K) = \\lambda\\). The above plot seems to agree with this, since the distribution is roughly symmetric about 10. The standard deviation of a poisson distribution is \\(\\sqrt{\\lambda}\\). In this case, one standard deviation would be \\(\\sqrt{10}\\approx3.16\\). How does the distribution change as we manipulate our parameter \\(\\lambda\\)? Applied to our example, how do the overall spread of adoptions change as the average number of adoptions change per month? pois_plot &lt;- function(x, lambda){ temp &lt;- tibble(num_adopt = x, density = dpois(x, lambda), cume_density = cumsum(density)) plot &lt;- ggplot(temp, aes(x = num_adopt, y = density)) + geom_bar(stat = &quot;identity&quot;, width = 0.2) + labs(x = NULL, y = NULL) return(plot) } adoptplot_1 &lt;- pois_plot(0:40, 2) + ggtitle(&quot;Lambda = 2&quot;) adoptplot_2 &lt;- pois_plot(0:40, 3) + ggtitle(&quot;Lambda = 3&quot;) adoptplot_3 &lt;- pois_plot(0:40, 5) + ggtitle(&quot;Lambda = 5&quot;) adoptplot_4 &lt;- pois_plot(0:40, 10) + labs(title = &quot;Lambda = 10&quot;, x = &quot;Number of Adoptions&quot;, y = &quot;Density&quot;) plot_grid(NULL, adoptplot_4, NULL, adoptplot_1, adoptplot_2, adoptplot_3, nrow = 2, ncol = 3) As you the plots above illustrate, the poisson distribution becomes more symmetric as \\(\\lambda\\) increases. When \\(\\lambda\\) is relatively small, the distribution is more right-skewed. "],["DiscreteContinuous.html", "Chapter 5 From Discrete to Continous Cases 5.1 Normal Approximation of Binomial Distribution 5.2 Normal Distribution", " Chapter 5 From Discrete to Continous Cases 5.1 Normal Approximation of Binomial Distribution In a way, the binomial distribution is the parent of the normal distribution. Ill explain. Several centuries ago, mathematician Abraham DeMoivre was asked to solve a gambling game in which one flips a coin many times and counts the number of heads. After repeatedly playing the game, he found that his results resembled a unique bellcurve shape. In this section, we will simulate his experiment: First, we will use R to create a coin. The heads side of our fair coin will be represented with a H and tails with a T. coin &lt;- c(&quot;H&quot;,&quot;T&quot;) Next, we will use sample to simulate random coin flips. The first argument of the function is for the elements of our random sampling process. The second size is the number of times that we will flip the coin. Finally, we will set replace to TRUE so that we keep both sides of our coin after flipping. flips &lt;- tibble(flip_num = 1:3600, outcome = sample(coin, size = 3600, replace = TRUE)) head(flips, 5) ## # A tibble: 5 x 2 ## flip_num outcome ## &lt;int&gt; &lt;chr&gt; ## 1 1 T ## 2 2 H ## 3 3 T ## 4 4 H ## 5 5 H Now, lets save the total number of heads. heads &lt;- flips %&gt;% filter(outcome == &quot;H&quot;) %&gt;% count() heads ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1794 Our last step is to find a way to repeat this game of 3600 flips. We will do so by creating a function. get_heads &lt;- function() { fair_coin &lt;- c(&quot;H&quot;, &quot;T&quot;) # Repeating code from the previous lines. flip &lt;- tibble(flip_num = 1:3600, outcome = sample(fair_coin, size = 3600, replace = TRUE)) heads_count &lt;- flip %&gt;% filter(outcome == &quot;H&quot;) %&gt;% count() return(heads_count) } Using this function, we will repeat the coin flip game 1000 times and count the number of heads using replicate. Read more about the functions documentation by typing ?replicate into the console. outcomes &lt;- tibble(game_num = 1:1000, heads = as.numeric(replicate(1000, get_heads(), simplify = TRUE))) head(outcomes, 5) ## # A tibble: 5 x 2 ## game_num heads ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1803 ## 2 2 1790 ## 3 3 1831 ## 4 4 1855 ## 5 5 1806 What does this look like? We will first save a ggplot object called demoivre_plot and then make a histogram. # Plotting our game outcomes demoivre_plot &lt;- ggplot(outcomes, aes(x = heads)) demoivre_plot + geom_histogram(bins = 25) + labs(x = &quot;Heads&quot;, y = &quot;Count&quot;) You can start to see the bell shape taking form. However, this is even easier to see with geom_density() which takes the same aesthetic mapping as geom_histogram. demoivre_plot + geom_density(alpha = 0.4, fill = &quot;lightsteelblue&quot;) + labs(x = &quot;Heads&quot;, y = &quot;Density&quot;) This unique bellcurve roughly follows the normal distribution. This distribution is useful beyond the coin flipping game. It is also explains many everyday phenomena such as heights, IQ scores, salaries, and blood pressure. Because of this, when we know that an outcomes, people, or observations are independent and random, we can make inferences about the larger picture. 5.2 Normal Distribution Now lets return to the normal distribution. We can define it as follows: \\[f(x)=(2\\pi)^{-1/2}e^{-x^2/2} \\quad \\textrm{for} \\enspace -\\infty &lt; x &lt; \\infty\\] The fact that \\(x\\) can take non-positive and non-integer values differentiates the normal distribution from discrete distributions {#Discrete}. These such distributions are considered continuous. In general, well define a The density function dnorm(x) can be thought of as f(x). The first argument x contains a vector of numbers that will yield their density. Lets try this with \\(x = 1.5\\) on a standard normal distribution. This means that the mean of the distribution is 0 and the standard deviation is 1. How likely is it that a point would be 1.5 standard deviations above the mean? dnorm(1.5, mean = 0, sd = 1) #The mean and sd arguments are set to these values by default. ## [1] 0.1295176 To interpret this density, simply think of {r}round(dnorm(1.5, 0, 1) as the probability that an observation would be 1.5 standard deviations greater than a mean. There are many real world scenarios in which we could apply this. For instance, the probability of an American male having a height that is 1.5 standard deviations greater than average (i.e. 76 inches or 6 feet and 3 inches) is about 13%. Oftentimes, we dont have a standardized distribution with 0 as the mean and 1 as the standard deviation. To demonstrate this, well return to our height example. The average height of men in the United States is 70 inches with 2 inches as the standard deviation. Thus, we must reevaluate the mean and sd arguments. normal &lt;- list(mean = 70, sd = 2) We can visualize this differently than discrete distributions. stat_function is a unique tool within ggplot that plots continuous curves. There are several required arguments: a geometry such as function, point, area, etc., a continuous function, lower and upper limits on the x and/or x axis, and a list of arguments for the function. We listed our arguments for a normal curve above. After plotting this curve, we will add a vertical line at the mean using geom_vline. For this geometry, we are required to provide an x-intercept. normal_plot &lt;- ggplot() + stat_function( geom = &quot;function&quot;, fun = dnorm, xlim = c(61, 79), args = normal # This is the list of arguments from above ) + geom_vline(xintercept = 70, color = &quot;lightsteelblue&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;Height (inches)&quot;, y = &quot;Density&quot;) normal_plot Unlike the geometric and binomal distributions, the normal distribution is symmetric about the mean. When a height is much greater or smaller than the average, it is less likely to occur. Now that we have made our density plot, we can again capture an interval of values in the distribution with our p distribution function pnorm(). If the shortest Duke basketball player (in the 2020-21 season) is 6 feet or 72 inches tall, what percentage of the American male population is shorter than the basketball team? round(pnorm(72, 70, 2), digits = 3) * 100 # Multiply by 100 to get a percentage ## [1] 84.1 To understand what the lower 84.1% of the normal distribution looks like, we will create two geometries with stat_function. The first will shade the entire distribution gray. On top of that, the second stat_function geometry fills the area of the distribution lower than 72 inches blue. ggplot() + stat_function(fun = dnorm, # Plot the entire distribution geom = &quot;area&quot;, fill = &quot;lightgray&quot;, color = &quot;black&quot;, xlim = c(61, 79), args = normal) + stat_function(fun = dnorm, # Change the color of heights &lt; 72 inches. geom = &quot;area&quot;, fill = &quot;steelblue&quot;, xlim = c(61, 72), args = normal ) + labs(x = &quot;Height (inches)&quot;, y = &quot;Density&quot;) As the blue portion of the plot makes clear, a majority of American men have heights below 72 inches. We use rnorm whenever we want to draw random samples from the normal distribution. On its own, the function helps us better understand the natural variation of real world processes that draw from normal distributions. If we repeat this sampling many times, well have a plot that more closely resembles the true distribution. Lets apply this concept again to height. Imagine that you took a random sample of 50 US men and took their height. # Take sample sample_50 &lt;- tibble(deviates = rnorm(50, 70, 2)) # Plot as a histogram ggplot(sample_50, aes(x = deviates)) + geom_histogram(bins = 20) If we took a larger sample, we would get something that looks more normally distributed. sample_5000 &lt;- tibble(deviates = rnorm(5000, 70, 2)) ggplot(sample_5000, aes(x = deviates)) + geom_histogram(bins = 20) The plots below will more finely measure this progression as sample sizes increase: # Create a function that randomly samples from normal distribution norm_plot &lt;- function(mean, sd, n){ temp &lt;- tibble(deviates = rnorm(n, mean, sd)) plot &lt;- ggplot(temp, aes(x = deviates)) + geom_histogram(bins = 20) return(plot) } # Produce plots sample_50 &lt;- norm_plot(70, 2, 50) sample_500 &lt;- norm_plot(70, 2, 500) sample_5000 &lt;- norm_plot(70, 2, 5000) # Plot together cowplot::plot_grid(NULL, normal_plot, NULL, sample_50, sample_500, sample_5000, ncol = 3, nrow = 2) Now that we have looked at the normal distribution, lets continue to look at other continuous random variables. "],["Continuous.html", "Chapter 6 Other Continuous Distributions 6.1 t-Distribution 6.2 Exponential Distribution 6.3 Gamma Distribution 6.4 2 Distribution", " Chapter 6 Other Continuous Distributions 6.1 t-Distribution We often dont know the true variance (and thus the standard deviation) of normally-distributed populations. However, we can still make estimations and inferences by adding a new parameter \\(t\\), the degrees of freedom. After doing this, we have created a t-distribution. This is also known as a students t-distribution. Simply put, degrees of freedom equals the number of observations in our sample that are able to, or have the freedom, to vary given our constraints. Since the only parameter that limits the freedom of our sample is the mean, we have \\(t = n-1\\) degrees of freedom in a t-distribution. Lets say that you have a sample of 5 observations: 1, 3, 5, 7, and an unknown number \\(x_5\\). Lets say we also know that the mean of the data is \\(\\bar{x}=5\\). Therefore, we can conclude using the definition of means that the \\(x_5=9\\). \\[\\begin{split} \\bar{x} &amp;= \\frac{x_1+x_2+\\ldots+x_p}{p} \\\\ 5 &amp;= \\frac{1+3+5+7+x_5}{5} \\\\ x_5 &amp;= 9 \\end{split}\\] In other words, we can know the value of all but 1 observation in our sample to be able to estimate the mean of a distribution. Since \\(n\\) is the sample size, having samples means more degrees of freedom. Regardless of the degrees of freedom a t-distribution has, it will look like an approximately-normal curve with greater variability than the normal distribution when plotted. This why some describe the distribution as having thick tails. ggplot() + geom_function(fun = dt, args = list(df = 5), color = &quot;darkred&quot;) + xlim(c(-3, 3)) + labs(y = &quot;Density&quot;) Figure 6.1: t-distribution with 5 degrees of freedom` I dont recommend that you use ggplot without something in the data argument since it limits your ability to alter aesthetics. However, for the purpose of simply visualizing continuous curves like the t-distribution, we can use geom_function. As you see above, we need to provide geom_function a function as the first argument. dt is the density function for the t-distribution. By specifying the limits on the x-axis with xlim to -3 and 3, we isolated the visualization to the most important part of the curve. As mentioned earlier, a t-distribution gains more degrees of freedom as the sample size increases. If we plot several curves with different degrees of freedom next to each other, we would notice that t-distributions with larger degrees of freedom have narrower tails and look closer to a normal distribution. Why is this the case? Imagine that you are working for a political campaign and want to measure the strength of your candidate against others in a poll. Assuming that your sample for the poll is random, you will have a poll more representative of the population as your sample size increases. As a result, polls with larger and more representative samples are usually more accurate. The impact of each respondents opinion on the result of the poll decreases, meaning that the variability of distribution decreases. The change in variability as sample sizes increase is what gives some t-distributions thinner tails. Below, well plot several different t-distributions with various degrees of freedom as well as a normal distribution. # Make a tibble or dataframe with five curves df &lt;- tibble(x = seq(-3, 3, by = 0.001), t1 = dt(x, 3), t2 = dt(x, 5), t3 = dt(x, 7), norm = dnorm(x)) %&gt;% pivot_longer(cols = 2:5, names_to = &quot;Distribution&quot;, values_to = &quot;Density&quot;) # Transform into three columns # Plot ggplot(df, aes(x = x, y = Density)) + geom_line(aes(color = Distribution)) + scale_color_viridis_d(labels = c(&quot;Normal&quot;, &quot;t, df = 3&quot;, &quot;t, df = 5&quot;, &quot;t, df = 7&quot;)) + xlim(c(-3, 3)) The concept of degrees of freedom alone can be covered more in-depth here. 6.2 Exponential Distribution Recall the earlier section in which we discussed Poisson {#Poisson} processes. The exponential distribution models a waiting time with \\(\\lambda\\) as the rate. This is similar to the geometric {# geometric} distribution, which models the number of failures until one success; however, in this case, we are considering events and not successes/failures. If \\(\\lambda\\) is the rate of a Poisson process, the density funtion can be written as: \\[f(y)=\\lambda e^{-\\lambda y} \\quad \\textrm{for} \\enspace y&gt;0\\] Thus, the probability that \\(Y=y\\) can be interpreted as the likelihood that two events ocurred in y units of time. Lets return to the adoption center example. Recall that the center has 10 adoptions per month on average. What is the probability that two pets are adopted in a week? If each month has 30 days then people are adopting \\(1/3\\) of a pet each day on average (even though you cant adopt a fraction of an animal!). We can then determine the rate as \\(\\lambda=1/3\\). Plugging the parameter \\(y&lt;7\\) and \\(\\lambda=1/3\\): \\[\\begin{split} P(Y &lt; 7) &amp;= \\int_{0}^{7} \\frac{1}{3} e^{-1/3y}dy \\\\ &amp;= \\frac{1}{3} \\int_{0}^{7} e^{-1/3y}dy \\\\ &amp;= \\frac{1}{3} [-3e^{-1/3y}]^{7}_{0} \\\\ &amp;= 1-\\frac{1}{e^{7/3}} \\approx 0.903 \\end{split}\\] Our final answer can be thought of as: There is a 90.3% chance probability that two animals are adopted before the end of a week. If you prefer to avoid calculus, you can also use R to answer this problem using pexp, the corresponding distribution function. pexp(7, rate = 1/3, lower.tail = TRUE) # i.e. P(Y &lt; 7) since lower.tail = TRUE ## [1] 0.903028 Why cant we we simply find the the sum of the densities 1:6? Remember that the exponential distribution is continuous, not discrete! So only calculating the individual densities at integer values would give us an underestimate. sum(dexp(1:6, 1/3)) ## [1] 0.7285453 Instead, we are required to provide an integral. To write a definite integral using R, we must provide a function and the two bounds. exp_fun &lt;- function(y) 1/3*exp(-1/3*y) # Write exponential function integrate(exp_fun, 0, 7) ## 0.903028 with absolute error &lt; 1e-14 How can we visualize our interval between 0 and 7 days? Lets again use stat_function to draw a curve. exp_correct &lt;- ggplot() + stat_function( fun = dexp, args = list(rate = 1/3), xlim = c(0, 7), geom = &quot;area&quot;, fill = &quot;skyblue1&quot; ) + stat_function( fun = dexp, args = list(rate = 1/3), # Remember, the arguments must be provided in a list xlim = c(0, 30), geom = &quot;function&quot;, # Choose a curve as the geometry size = 0.5) + geom_segment(aes(x = 0:30, y = 0, xend = 0:30, yend = dexp(0:30, rate = 1/3)), size = 0.5) + labs(x = &quot;Days Between Adoptions&quot;, y = &quot;Density&quot;) Notice how the plot on the left looks much better? They are also placed so that the blue fill does not cover the curve? The jagged edges on the right plot results from placing the function geometry before the area geometry. Because a large majority of the plot below 7 is colored in, it is quite likely that 2 pets would be adopted in a week. As the number of days waiting approaches 30, the probability approaches 0. By what point after one adoption can we begin expecting another? To answer this question, we can look for the median of the distribution. Recall that q functions return a x-value after providing a quantile in the distribution. Here, well use qexp. qexp(0.5, rate = 1/3) ## [1] 2.079442 The other measure of center for an exponential distribution is the mean. This is much easier to solve; if K is a Poisson random variable, \\(E(K)=\\frac{1}{\\lambda}\\). So on average, we can expect an adoption on day \\(\\frac{1}{1/3}=3\\). Imagine that you are working for a rescue shelter for pets and want to present the information that you just learnedthat there are about 3 days on average between any two adoptions in a month. Well build a deliverable visualization using the power of ggplot, ggimage, as well as your knowledge about the exponential distribution. First, load the new ggimage package. This allows us to include image files and other graphic objects in our visualizations. To learn more about it, you can read ggimages documentation here. library(ggimage) # Remember to use install.packages(&quot;ggimage&quot;) first In particular, well need the package to include emojis of dogs and illustrate the average time period between adoptions. Think of adding the emojis as adding any other geometry. Beyond a coordinate position, the geom_emoji function requires an aesthetic called image. The special unicode required for dogs is 1f415. We can also set the size of the image similar to geometries like geom_point.  ggplot() + stat_function( fun = dexp, args = list(rate = 1/3), xlim = c(0, 3), geom = &quot;area&quot;, fill = &quot;skyblue1&quot; ) + stat_function( fun = dexp, args = list(rate = 1/3), xlim = c(0, 5), geom = &quot;function&quot;, size = 0.5) + geom_segment(aes(x = 0:5, y = 0, xend = 0:5, yend = dexp(0:5, rate = 1/3)), size = 0.5) + labs(x = &quot;Days Between Adoptions&quot;, y = &quot;Probability&quot;) + geom_emoji(aes(x = c(0.05,3), y = c(dexp(c(0,3), rate = 1/3)) + 0.025, image = &quot;1f415&quot;), size = 0.1) + annotate(geom = &quot;text&quot;, x = 2.75, y = dexp(1, 1/3) + 0.01, label = &quot;On average, two pets are adopted every three days&quot;) + theme_classic() 6.3 Gamma Distribution The exponential distribution is quite similar to the geometric distribution {#geometric} since they both consider the amount of time it takes for one event to occur. The main difference between the two is that geometric random variables model Bernoulli or binary processes and exponential random variables model poisson ones. Bernoulli processes consider independent trials, so if each trial is a day then we would ignore the possibility of having more than one adoption or success per day. Some might consider changing the trial unit to each animal in the shelter, but we cannot determine the probability of each animal being adopted, and we also cant assume that they each animals probability is the same. If \\(N\\) is based on poisson processses and has parameter \\(\\lambda\\), the rate, \\(N\\) represents the amount of time we must wait until \\(r\\) events, thus following the gamma distribution. \\[ P(N=n)=\\frac{\\lambda^r}{\\Gamma(r)}n^{r-1}e^{-\\lambda n} \\quad \\textrm{for} \\enspace k&gt;0\\] If the \\(\\Gamma\\) (Gamma) function in the equation looks new to you, you can read more about it here. When \\(r=1\\) the equation simplifies to the exponential distribution, making it a special case similar to the geometric distribution. \\[ \\begin{split}f(k)&amp;=\\frac{\\lambda^1}{\\Gamma(1)}k^{1-1}e^{-\\lambda k} \\\\ &amp;= \\frac{\\lambda}{1}k^0e^{-\\lambda k} \\\\ &amp;= \\lambda e^{-\\lambda k} \\end{split}\\] Returning to the adoption example, the gamma distribution lets us figure out the number of days that it takes for more than one adoption to occur in an interval of time. For example, what is the probability that it takes less than 2 weeks for 3 pets to be adopted if 10 pets are adopted every month? Recall that \\(\\lambda = 1/3\\), assuming that an adoption is equally likely to occur every day of the month. Therfore, if \\(K\\) is a gamma random variable for the number of days it takes for 3 adoptions to occur, we can write the probability of \\(K&lt;14 \\enspace \\textrm{days}\\) \\[ P(K&lt;14) = \\int_0^{14} \\frac{{1/3}^3}{\\Gamma(3)}k^{3-1}e^{-k/3}dk \\approx 0.844\\] Or, two ways using R: # Using the gamma distribution function pgamma(14, shape = 3, rate = 1/3) ## [1] 0.8443188 # Is the same as writing this definite integral gamma_fun &lt;- function(k, lambda = 1/3, r = 3) (lambda^r)/(gamma(r)) * k^(r-1) * exp(-lambda * k) integrate(gamma_fun, 0, 14) ## 0.8443188 with absolute error &lt; 9.4e-15 So there is an approximate 84.4% probability that the center has 3 adoptions in two weeks. How do you think that this probability changes as we alter \\(r\\) or \\(\\lambda\\)? # Create a vector of &quot;day&quot; values x &lt;- seq(0, 30, by = 0.01) # Create a tibble with four gamma distributions, each with different r and lambda values df &lt;- tibble(x, gamma_1 = dgamma(x, 3, 1/3), gamma_2 = dgamma(x, 5, 1/3), gamma_3 = dgamma(x, 5, 2/3), gamma_4 = dgamma(x, 5, 1/6)) %&gt;% # Transform the data frame into longer format pivot_longer(cols = 2:5, names_to = &quot;Distribution&quot;, values_to = &quot;Density&quot;) # Now plot! ggplot(df, aes(x = x, y = Density, color = Distribution)) + geom_line(aes(group = Distribution)) + labs(x = &quot;Days&quot;, y = &quot;Density&quot;) + scale_color_viridis_d(labels = c(&quot;r = 3, lambda = 1/3&quot;, &quot;r = 5, lambda = 1/3&quot;, &quot;r = 5, lambda = 2/3&quot;, &quot;r = 5, lambda = 1/6&quot;)) !! write something about viridis palletes being colorblind-friendly. 6.4 2 Distribution "]]
